{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# the imutils package is a collection of convenience functions\n",
    "# to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and both Python 2.7 and Python 3\n",
    "import skimage.io as io\n",
    "from imutils import paths\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "# for io.imshow we need to import it from skimage\n",
    "# the syntax is import skimage.io as io\n",
    "# linearsvc is a linear support vector machine and needs to be imported from sklearn.svm\n",
    "# the syntax is from sklearn.svm import LinearSVC\n",
    "# labelencoder is a label encoder and needs to be imported from sklearn.preprocessing\n",
    "# the syntax is from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "# import sklearn.svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# for pickle we need to import it from sklearn.externals\n",
    "# the syntax for pickle is import pickle\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will try SIFT\n",
    "def sift(image):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # detect keypoints in the image\n",
    "    descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "    # convert the keypoints from KeyPoint objects to NumPy\n",
    "    # arrays\n",
    "    kps = np.float32([kp.pt for kp in kps])\n",
    "    # return a tuple of keypoints and features\n",
    "    return (kps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3308\\3527507304.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[0msift_feature_extraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3308\\3527507304.py\u001b[0m in \u001b[0;36msift_feature_extraction\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[1;31m# detect keypoints in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# perform SIFT feature extraction\n",
    "def sift_feature_extraction():\n",
    "    for i in range(0, 6):\n",
    "        try:\n",
    "            os.remove(\"_SIFTFeatures_m_\" + str(i) + \".csv\")\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(\"_SIFTFeatures_w_\" + str(i) + \".csv\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    f0_m = open(\"_SIFTFeatures_m_0.csv\", \"a\", newline='')\n",
    "    f1_m = open(\"_SIFTFeatures_m_1.csv\", \"a\", newline='')\n",
    "    f2_m = open(\"_SIFTFeatures_m_2.csv\", \"a\", newline='')\n",
    "    f3_m = open(\"_SIFTFeatures_m_3.csv\", \"a\", newline='')\n",
    "    f4_m = open(\"_SIFTFeatures_m_4.csv\", \"a\", newline='')\n",
    "    f5_m = open(\"_SIFTFeatures_m_5.csv\", \"a\", newline='')\n",
    "\n",
    "    f0_w = open(\"_SIFTFeatures_w_0.csv\", \"a\", newline='')\n",
    "    f1_w = open(\"_SIFTFeatures_w_1.csv\", \"a\", newline='')\n",
    "    f2_w = open(\"_SIFTFeatures_w_2.csv\", \"a\", newline='')\n",
    "    f3_w = open(\"_SIFTFeatures_w_3.csv\", \"a\", newline='')\n",
    "    f4_w = open(\"_SIFTFeatures_w_4.csv\", \"a\", newline='')\n",
    "    f5_w = open(\"_SIFTFeatures_w_5.csv\", \"a\", newline='')\n",
    "\n",
    "    f_m = [f0_m , f1_m, f2_m, f3_m, f4_m, f5_m]\n",
    "    f_w = [f0_w , f1_w, f2_w, f3_w, f4_w, f5_w]\n",
    "\n",
    "    for imagePath in paths.list_images(\"dataset/Woman\"):\n",
    "        # extract the label from the image path\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        # load the image, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(imagePath)\n",
    "        org_image = cv2.imread(imagePath)\n",
    "        \n",
    "        # extract Histogram of Oriented Gradients from the\n",
    "        # test image\n",
    "        # display the original image\n",
    "        # if the image exists, display it and count the number of images\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (200, 200))\n",
    "            org_image = cv2.resize(org_image, (200, 200))\n",
    "            # detect keypoints in the image\n",
    "            sift = cv2.SIFT_create()\n",
    "            kps = sift.detect(image, None)\n",
    "            #descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            #(kps, features) = descriptor.detectAndCompute(image, None)\n",
    "            # convert the keypoints from KeyPoint objects to NumPy arrays\n",
    "            img = cv2.drawKeypoints(image,kps,org_image)\n",
    "            plt.imshow(img)\n",
    "            kps = np.float32([kp.pt for kp in kps])\n",
    "            csv.writer(f_w[(int)(label)]).writerow([label, kps])\n",
    "\n",
    "    for imagePath in paths.list_images(\"dataset/men\"):\n",
    "        # extract the label from the image path\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        # load the image, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        # extract Histogram of Oriented Gradients from the\n",
    "        # test image\n",
    "        # display the original image\n",
    "        # if the image exists, display it and count the number of images\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (200, 200))\n",
    "            # detect keypoints in the image\n",
    "            sift = cv2.SIFT_create()\n",
    "            kps = sift.detect(image, None)\n",
    "            #descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "            #(kps, features) = descriptor.detectAndCompute(image, None)\n",
    "            # convert the keypoints from KeyPoint objects to NumPy arrays\n",
    "            kps = np.float32([kp.pt for kp in kps])\n",
    "            csv.writer(f_m[(int)(label)]).writerow([label, kps])\n",
    "        \n",
    "    for file in f_w:\n",
    "        file.close()\n",
    "    for file in f_m:\n",
    "        file.close()\n",
    "\n",
    "sift_feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.671886 53.839962]\n"
     ]
    }
   ],
   "source": [
    "content = pd.read_csv(\"SIFTFeatures_m_\" + str(0) + \".csv\", header=None, skiprows = 0, nrows= 80, sep=',')\n",
    "content = np.array(content)\n",
    "content = content[0,1]\n",
    "content = content.replace(\"\\n\", \"\").replace(\"[[\", \"\").replace(\"]]\", \"\").replace(\"[ \", \"[\").replace(\" ]\", \"]\" )\n",
    "content = content.split(\"] [\")\n",
    "content = np.array(content)\n",
    "if(content.__contains__('')):\n",
    "    content = np.delete(content, np.where(content == ''))\n",
    "\n",
    "content_new = []\n",
    "for c in content:\n",
    "    c = c.split(\" \")\n",
    "    c = np.array(c)\n",
    "    if(c.__contains__('')):\n",
    "        c = np.delete(c, np.where(c == ''), axis=0)\n",
    "    c = np.array(c, dtype= np.float32)\n",
    "    content_new.append(c)\n",
    "\n",
    "print(content_new[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sift():\n",
    "    k = 0\n",
    "    #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "    model = SVC(kernel='rbf', gamma=0.01, C=1000) # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "    for m in range (3):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for i in range (0, 6):\n",
    "            try:\n",
    "                labels_data = pd.read_csv(\"hogFeatures_m_\" + str(i) + \".csv\", header=None, skiprows = k, nrows= 80, sep=',')\n",
    "            except:\n",
    "                labels_data = []\n",
    "                pass\n",
    "            data_temp = np.array(labels_data)\n",
    "            for j in range (len(labels_data)):\n",
    "                temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "                temp = temp.replace(\"\\n\", \"\")\n",
    "                temp = temp.replace(\"]\", \"\")\n",
    "                temp= temp.split(\" \")\n",
    "                temp = np.array(temp)\n",
    "                if(temp.__contains__('')):\n",
    "                    temp = np.delete(temp, np.where(temp == ''))\n",
    "                data.append(np.array(temp, dtype=np.float32))\n",
    "                labels.append(labels_data.iloc[j, 0])\n",
    "\n",
    "            try:\n",
    "                labels_data = pd.read_csv(\"hogFeatures_w_\" + str(i) + \".csv\", header=None, skiprows = k, nrows= 80, sep=',')\n",
    "            except:\n",
    "                labels_data = []\n",
    "                pass\n",
    "            data_temp = np.array(labels_data)\n",
    "            for j in range (len(labels_data)):\n",
    "                temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "                temp = temp.replace(\"\\n\", \"\")\n",
    "                temp = temp.replace(\"]\", \"\")\n",
    "                temp= temp.split(\" \")\n",
    "                temp = np.array(temp)\n",
    "                if(temp.__contains__('')):\n",
    "                    temp = np.delete(temp, np.where(temp == ''))\n",
    "                #print(data_temp)\n",
    "                data.append(np.array(temp, dtype=np.float32))\n",
    "                labels.append(labels_data.iloc[j, 0])\n",
    "        \n",
    "        k += 81\n",
    "        data = np.array(data, dtype=\"float\")\n",
    "        labels = LabelEncoder().fit_transform(labels)\n",
    "        # # partition the data into training and testing splits, using 75%\n",
    "        trainingData, testData, trainingLabels, testLabels = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "        \n",
    "\n",
    "        print(\"[INFO] evaluating...\", flush=True)\n",
    "        \n",
    "        # train the model\n",
    "        # how to continue to train SVM based on the previous model\n",
    "        # this is done by using the partial_fit method\n",
    "        # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "        model.fit(trainingData, trainingLabels)\n",
    "\n",
    "        # evaluate the model and update the accuracies list\n",
    "        acc = model.score(testData, testLabels)\n",
    "        print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "        # dump the classifier to file\n",
    "        print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "        f = open(\"SVM_classifier\" + str(i) + \".pkl\", \"wb\") # wb = write binary\n",
    "        f.write(pickle.dumps(model))\n",
    "        f.close()\n",
    "\n",
    "train_model_sift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hog implemented in skimage.feature\n",
    "def get_hog():\n",
    "    # get images from images array, convert to grayscale\n",
    "    # and resize to 200x200\n",
    "    \n",
    "    # remove the files if exist\n",
    "    for i in range(0, 6):\n",
    "        try:\n",
    "            os.remove(\"hogFeatures_m_\" + str(i) + \".csv\")\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(\"hogFeatures_w_\" + str(i) + \".csv\")\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "    f0_m = open(\"hogFeatures_m_0.csv\", \"a\", newline='')\n",
    "    f1_m = open(\"hogFeatures_m_1.csv\", \"a\", newline='')\n",
    "    f2_m = open(\"hogFeatures_m_2.csv\", \"a\", newline='')\n",
    "    f3_m = open(\"hogFeatures_m_3.csv\", \"a\", newline='')\n",
    "    f4_m = open(\"hogFeatures_m_4.csv\", \"a\", newline='')\n",
    "    f5_m = open(\"hogFeatures_m_5.csv\", \"a\", newline='')\n",
    "\n",
    "    f0_w = open(\"hogFeatures_w_0.csv\", \"a\", newline='')\n",
    "    f1_w = open(\"hogFeatures_w_1.csv\", \"a\", newline='')\n",
    "    f2_w = open(\"hogFeatures_w_2.csv\", \"a\", newline='')\n",
    "    f3_w = open(\"hogFeatures_w_3.csv\", \"a\", newline='')\n",
    "    f4_w = open(\"hogFeatures_w_4.csv\", \"a\", newline='')\n",
    "    f5_w = open(\"hogFeatures_w_5.csv\", \"a\", newline='')\n",
    "\n",
    "    f_m = [f0_m , f1_m, f2_m, f3_m, f4_m, f5_m]\n",
    "    f_w = [f0_w , f1_w, f2_w, f3_w, f4_w, f5_w]\n",
    "\n",
    "    for imagePath in paths.list_images(r\"./Image-Segmentation2/WOMEN\"):\n",
    "        # extract the label from the image path\n",
    "        label = imagePath.split(os.path.sep)[-1]\n",
    "        \n",
    "        # load the image, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        # extract Histogram of Oriented Gradients from the\n",
    "        # test image\n",
    "        # display the original image\n",
    "        # if the image exists, display it and count the number of images\n",
    "        if image is not None:\n",
    "            (H, hogImage) = feature.hog(image, orientations=15, pixels_per_cell=(10, 10), cells_per_block=(2, 2),channel_axis=-1, transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "            # print(label)\n",
    "            csv.writer(f_w[(int)(label[0])]).writerow([label, H])\n",
    "\n",
    "    for imagePath in paths.list_images(r\"./Image-Segmentation2/MEN\"):\n",
    "        # extract the label from the image path\n",
    "        label = imagePath.split(os.path.sep)[-1]\n",
    "        \n",
    "        # load the image, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        # extract Histogram of Oriented Gradients from the\n",
    "        # test image\n",
    "        # display the original image\n",
    "        # if the image exists, display it and count the number of images\n",
    "        if image is not None:\n",
    "            (H, hogImage) = feature.hog(image, orientations=15, pixels_per_cell=(10, 10), cells_per_block=(2, 2),channel_axis=-1, transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "            csv.writer(f_m[(int)(label[0])]).writerow([label, H])\n",
    "        \n",
    "    for file in f_w:\n",
    "        file.close()\n",
    "    for file in f_m:\n",
    "        file.close()\n",
    "\n",
    "get_hog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>0_men (99).JPG</td>\n",
       "      <td>[0.00000000e+00 0.00000000e+00 0.00000000e+00 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                                                  1\n",
       "count                1                                                  1\n",
       "unique               1                                                  1\n",
       "top     0_men (99).JPG  [0.00000000e+00 0.00000000e+00 0.00000000e+00 ...\n",
       "freq                 1                                                  1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_data = pd.read_csv(\"hogFeatures_m_\" + str(0) + \".csv\", header=None, skiprows = 171, nrows= 1000, sep=',')\n",
    "print(len(labels_data))\n",
    "labels_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encode the labels, converting them from strings to integers\n",
    "# the LabelEncoder class is used to normalize labels\n",
    "\n",
    "# read the labels and data from the csv files and store them in data and labels\n",
    "# read only 80 row from each file in each iteration to avoid memory error\n",
    "# in each iteration we will run train_test_split on the data and labels\n",
    "# and train the model on the training data and test it on the test data\n",
    "# and store the accuracy in the accuracies array\n",
    "# at the end we will print the average accuracy\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "def train_model():\n",
    "    k = 0\n",
    "    #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "    # model = SVC(kernel='rbf', gamma=0.01, C=10) # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "    model = SGDClassifier(loss=\"hinge\", max_iter=100)\n",
    "    \n",
    "    for m in range (3):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for i in range (0, 6):\n",
    "            try:\n",
    "                labels_data = pd.read_csv(\"hogFeatures_m_\" + str(i) + \".csv\", header=None, skiprows = k, nrows= 80, sep=',')\n",
    "            except:\n",
    "                labels_data = []\n",
    "                pass\n",
    "            data_temp = np.array(labels_data)\n",
    "            for j in range (len(labels_data)):\n",
    "                temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "                temp = temp.replace(\"\\n\", \"\")\n",
    "                temp = temp.replace(\"]\", \"\")\n",
    "                temp= temp.split(\" \")\n",
    "                temp = np.array(temp)\n",
    "                if(temp.__contains__('')):\n",
    "                    temp = np.delete(temp, np.where(temp == ''))\n",
    "                data.append(np.array(temp, dtype=np.float32))\n",
    "                labels.append(labels_data.iloc[j, 0])\n",
    "\n",
    "            try:\n",
    "                labels_data = pd.read_csv(\"hogFeatures_w_\" + str(i) + \".csv\", header=None, skiprows = k, nrows= 80, sep=',')\n",
    "            except:\n",
    "                labels_data = []\n",
    "                pass\n",
    "            data_temp = np.array(labels_data)\n",
    "            for j in range (len(labels_data)):\n",
    "                temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "                temp = temp.replace(\"\\n\", \"\")\n",
    "                temp = temp.replace(\"]\", \"\")\n",
    "                temp= temp.split(\" \")\n",
    "                temp = np.array(temp)\n",
    "                if(temp.__contains__('')):\n",
    "                    temp = np.delete(temp, np.where(temp == ''))\n",
    "                #print(data_temp)\n",
    "                data.append(np.array(temp, dtype=np.float32))\n",
    "                labels.append(labels_data.iloc[j, 0])\n",
    "        \n",
    "        k += 81\n",
    "        data = np.array(data, dtype=\"float\")\n",
    "        le = LabelEncoder()\n",
    "        labels = le.fit_transform(labels)\n",
    "        # # partition the data into training and testing splits, using 75%\n",
    "        trainingData, testData, trainingLabels, testLabels = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "        \n",
    "\n",
    "        print(\"[INFO] evaluating...\", flush=True)\n",
    "        \n",
    "        # train the model\n",
    "        # how to continue to train SVM based on the previous model\n",
    "        # this is done by using the partial_fit method\n",
    "        # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "        model.partial_fit(trainingData, trainingLabels,classes=[0,1,2,4,5])\n",
    "\n",
    "        # evaluate the model and update the accuracies list\n",
    "        acc = model.score(testData, testLabels)\n",
    "        print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "        # dump the classifier to file\n",
    "        print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "        f = open(\"SVM_classifier\" + str(i) + \".pkl\", \"wb\") # wb = write binary\n",
    "        f.write(pickle.dumps(model))\n",
    "        f.close()\n",
    "\n",
    "train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
