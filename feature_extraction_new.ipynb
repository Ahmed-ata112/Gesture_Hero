{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# the imutils package is a collection of convenience functions\n",
    "# to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and both Python 2.7 and Python 3\n",
    "import skimage.io as io\n",
    "from imutils import paths\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "from os import walk\n",
    "# for io.imshow we need to import it from skimage\n",
    "# the syntax is import skimage.io as io\n",
    "# linearsvc is a linear support vector machine and needs to be imported from sklearn.svm\n",
    "# the syntax is from sklearn.svm import LinearSVC\n",
    "# labelencoder is a label encoder and needs to be imported from sklearn.preprocessing\n",
    "# the syntax is from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "# import sklearn.svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# for pickle we need to import it from sklearn.externals\n",
    "# the syntax for pickle is import pickle\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import cv2 as cv2\n",
    "# import cv2.cv2 as cv2\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "import imageio\n",
    "from os import walk\n",
    "from pyefd import elliptic_fourier_descriptors\n",
    "from skimage import feature\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns;\n",
    "import collections\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import skew\n",
    "from skimage import segmentation\n",
    "from skimage.filters import sobel\n",
    "sns.set()\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hog implemented in skimage.feature\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_hog():\n",
    "#     # get images from images array, convert to grayscale\n",
    "#     # and resize to 200x200 \n",
    "\n",
    "#     # try:\n",
    "#     #     os.remove(\"HOG_PCA_FEATURES.csv\")\n",
    "#     # except OSError:\n",
    "#     #     pass  \n",
    "\n",
    "#     # features_file = open(\"HOG_PCA_FEATURES.csv\", \"w\", newline='')\n",
    "#     labels = []\n",
    "#     pca_features = []\n",
    "    \n",
    "\n",
    "\n",
    "#     for imagePath in os.listdir(\"Image-Segmentation2\"):\n",
    "#         # extract the label from the image path\n",
    "#         # we can get the label from the image name as the label is the first char of the image name\n",
    "#         # for example, the label of image 1_1.jpg is 1\n",
    "#         labels.append((int)(imagePath.split(os.path.sep)[-1][0]))\n",
    "        \n",
    "#         # load the image, convert it to grayscale, and detect\n",
    "#         # edges in it\n",
    "#         image = cv2.imread(\"Image-Segmentation/\" + imagePath)\n",
    "        \n",
    "#         # extract Histogram of Oriented Gradients from the\n",
    "#         # test image\n",
    "#         # display the original image\n",
    "#         # if the image exists, display it and count the number of images\n",
    "#         if image is not None:\n",
    "#             image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             image = cv2.resize(image, (461, 260))\n",
    "#             (H, hogImage) = feature.hog(image, orientations=9,  pixels_per_cell=(32, 32), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "#             # H = H.reshape(-1, 1)\n",
    "#             # pca = PCA(0.8).fit(H)\n",
    "#             # components = pca.transform(H)\n",
    "#             pca_features.append(H)\n",
    "\n",
    "            \n",
    "\n",
    "#             # store the label and features in the csv file\n",
    "#             #csv.writer(features_file).writerow([labels[-1], pca_features])\n",
    "\n",
    "#     return pca_features, np.array(labels)        \n",
    "\n",
    "def get_hog():\n",
    "    print(\"HOG\\n\")\n",
    "    file  = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "\n",
    "                image = cv2.imread(\"Image-Segmentation2/\" + arch)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                image = cv2.resize(image, (461, 260))\n",
    "                \n",
    "                (H, hogImage) = feature.hog(image, orientations=9,  pixels_per_cell=(32, 32), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "\n",
    "                    \n",
    "                file.write(name)\n",
    "                for item in range(len(H)):\n",
    "                    file.write(\",%.3f\" % H[item])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "def HOG_PCA():\n",
    "    data_HOG = pd.read_csv(r'./Feature-Extraction/Histogram-of-Oriented-Gradients.txt', sep=',', header=None)\n",
    "    file2 = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients-PCA.txt\", \"w\")\n",
    "    file2_3 = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients-PCA-2-3.txt\", \"w\")\n",
    "    file3_4 = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients-PCA-3-4.txt\", \"w\")\n",
    "    name_HOG = data_HOG.iloc[:, 0]\n",
    "    value_HOG = data_HOG.iloc[:, 1:-1]\n",
    "    tag_HOG = data_HOG.iloc[:, -1] # 0,1,2,3,4,5\n",
    "    print(\"PCA\")\n",
    "    pca = PCA(0.97).fit(value_HOG)\n",
    "    joblib.dump(pca, r\"./Feature-Extraction/pca.pkl\")\n",
    "    \n",
    "    components = pca.transform(value_HOG)\n",
    "    print(components.shape)\n",
    "    for row in range(len(components)):\n",
    "        if(name_HOG[row][0] == '2' or name_HOG[row][0] == '3'):\n",
    "            file2_3.write(name_HOG[row])\n",
    "            for colm in range(len(components[row])):\n",
    "                file2_3.write(\",%.4f\" %components[row][colm])\n",
    "            file2_3.write(\",%s\" %tag_HOG[row] + \"\\n\")\n",
    "        if(name_HOG[row][0] == '3' or name_HOG[row][0] == '4'):\n",
    "            file3_4.write(name_HOG[row])\n",
    "            for colm in range(len(components[row])):\n",
    "                file3_4.write(\",%.4f\" %components[row][colm])\n",
    "            file3_4.write(\",%s\" %tag_HOG[row] + \"\\n\")\n",
    "        file2.write(name_HOG[row])\n",
    "        for colm in range(len(components[row])):\n",
    "            file2.write(\",%.4f\" %components[row][colm])\n",
    "        file2.write(\",%s\" %tag_HOG[row] + \"\\n\")\n",
    "    file2.close()\n",
    "    file2_3.close()\n",
    "    file3_4.close()\n",
    "\n",
    "#pca_features, labels = get_hog()\n",
    "get_hog()\n",
    "HOG_PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the first method of classification we use Support Vector Machine \n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "base = \"./\"\n",
    "\n",
    "def SVMutilities(txt,test_percentage):\n",
    "    pathsvm = base + \"Feature-Extraction/SVM\"\n",
    "    \n",
    "    if not os.path.exists(pathsvm):\n",
    "        os.makedirs(pathsvm)\n",
    "        \n",
    "    data = pd.read_csv(base + '/Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    \n",
    "    # we shuffle it for better performance \n",
    "    # data=shuffle(data, random_state=42)\n",
    "    \n",
    "    s=data.shape\n",
    "    # print(s)\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "     \n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "    \n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "    \n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "    \n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "    \n",
    "    #print(data.tail())\n",
    "    \n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    allValuesName = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[allValuesName]\n",
    "    # print value type\n",
    "    # print(type(value))\n",
    "    # convert to float\n",
    "    # value=value.astype(float)\n",
    "    \n",
    "    tags=data[col[-1]] #columna de tags\n",
    "    \n",
    "    data['gender'] = data['NAME'].map(lambda x: 'woman' in x.lower())\n",
    "    \n",
    "    # i added a stratify to the train_test_split to make sure that the train and test sets have the same proportion of class labels as the input data\n",
    "    # its based on gender\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test_percentage,stratify=data['gender'], random_state=42)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=test_percentage,stratify=Y_train, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X_val, Y_val\n",
    "\n",
    "def SVM(txt,test_percentage):\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test, X_val, Y_val = SVMutilities(txt,test_percentage)\n",
    "    X_train2_3, X_test2_3, Y_train2_3, Y_test2_3, X_val2_3, Y_val2_3 = SVMutilities(txt+\"-2-3\",test_percentage)\n",
    "    X_train3_4, X_test3_4, Y_train3_4, Y_test3_4, X_val3_4, Y_val3_4 = SVMutilities(txt+\"-3-4\",test_percentage)\n",
    "\n",
    "    C_range=[0.01, 0.1, 1, 10, 100, 1000]\n",
    "    gamma_range=[1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5]\n",
    "    parameters= [\n",
    "        {\n",
    "            'kernel': ['rbf'],\n",
    "            'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "            'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "        }#, \n",
    "        #{\n",
    "        #    'kernel': ['linear'],\n",
    "        #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        #}, \n",
    "        #{\n",
    "        #    'kernel': ['sigmoid'],\n",
    "        #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "        #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        #}, \n",
    "        #{\n",
    "        #    'kernel': ['poly'],\n",
    "        #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "        #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        #}\n",
    "        \n",
    "    ]\n",
    "    # \n",
    "    clf =GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "    clf2_3 = GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "    clf3_4 = GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "    # pipe = Pipeline([('scaler', StandardScaler()), ('svm', clf)])\n",
    "    clf.fit(X_train.values,Y_train)\n",
    "    clf2_3.fit(X_train2_3.values,Y_train2_3)\n",
    "    clf3_4.fit(X_train3_4.values,Y_train3_4)\n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    scores2_3 = clf2_3.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    scores3_4 = clf3_4.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf2_3.best_params_, clf2_3.best_score_))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf3_4.best_params_, clf3_4.best_score_))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    plt.xlabel('Gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    fig=plt.title('Heat map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    fig.get_figure().savefig(base + r'Feature-Extraction/SVM/Heatmap-'+txt+'-'+str(int(test_percentage*100))+'%.jpg')\n",
    "    plt.show()\n",
    "    print(clf.best_params_)#mejor parametro\n",
    "\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for m, s, p in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    \n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    target_names_2_3= [\"2\",\"3\"]\n",
    "    target_names_3_4= [\"3\",\"4\"]\n",
    "    \n",
    "    table=classification_report(Y_val,y_pred, target_names=target_names)\n",
    "    table2_3=classification_report(Y_val2_3,clf2_3.predict(X_val2_3), target_names=target_names_2_3)\n",
    "    table3_4=classification_report(Y_val3_4,clf3_4.predict(X_val3_4), target_names=target_names_3_4)\n",
    "    table=str(table)\n",
    "    table2_3=str(table2_3)\n",
    "    table3_4=str(table3_4)              \n",
    "    print(table)\n",
    "    print(table2_3)\n",
    "    print(table3_4)\n",
    "    \n",
    "    mat=confusion_matrix(Y_val, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    Matrizconf.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-'+str(int(test_percentage*100))+'%.png')\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat2_3=confusion_matrix(Y_val2_3, clf2_3.predict(X_val2_3))\n",
    "    Matrizconf2_3=sns.heatmap(mat2_3.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "                xticklabels=target_names_2_3, yticklabels= target_names_2_3 )\n",
    "    \n",
    "    mat2_3=plt.title('Confusion map '+txt+'-2-3-'+str(int(test_percentage*100))+'%')\n",
    "    Matrizconf2_3.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-2-3-'+str(int(test_percentage*100))+'%.png')\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat3_4=confusion_matrix(Y_val3_4, clf3_4.predict(X_val3_4))\n",
    "    Matrizconf3_4=sns.heatmap(mat3_4.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "                xticklabels=target_names_3_4, yticklabels= target_names_3_4 ) \n",
    "    mat3_4=plt.title('Confusion map '+txt+'-3-4-'+str(int(test_percentage*100))+'%')\n",
    "    Matrizconf3_4.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-3-4-'+str(int(test_percentage*100))+'%.png')\n",
    "    \n",
    "    \n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_val.groupby(Y_val).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "    joblib.dump(clf,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    joblib.dump(clf2_3,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-2-3-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    joblib.dump(clf3_4,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-3-4-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    \n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(clf.score)\n",
    "    print(\"Accuracy: \"+str(clf.score(X_test,Y_test)))\n",
    "    print(\"Accuracy: \"+str(clf2_3.score(X_test2_3,Y_test2_3)))\n",
    "    print(\"Accuracy: \"+str(clf3_4.score(X_test3_4,Y_test3_4)))\n",
    "\n",
    "    return X_test, Y_test\n",
    "    \n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# SVM(\"Elliptic-Fourier\",0.2)\n",
    "# SVM(\"Geometric\" ,0.2)\n",
    "x_test, y_test = SVM(\"Histogram-of-Oriented-Gradients-PCA\" ,0.2)\n",
    "# SVM(\"Cof\" ,0.2)\n",
    "# SVM(\"VHIST\" ,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def SVM(txt,test_percentage):\n",
    "#     data, value, tags = SVMutilities(txt,test_percentage)\n",
    "#     data2_3, value2_3, tags2_3 = SVMutilities(txt+\"-2-3\",test_percentage)\n",
    "#     data3_4, value3_4, tags3_4 = SVMutilities(txt+\"-3-4\",test_percentage)\n",
    "    \n",
    "#     # i added a stratify to the train_test_split to make sure that the train and test sets have the same proportion of class labels as the input data\n",
    "#     # its based on gender\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test_percentage,stratify=data['gender'], random_state=42)\n",
    "#     X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=test_percentage,stratify=Y_train, random_state=42)\n",
    "#     X_train2_3, X_test2_3, Y_train2_3, Y_test2_3 = train_test_split(value2_3, tags2_3, test_size=test_percentage,stratify=data2_3['gender'], random_state=42)\n",
    "#     X_train2_3, X_val2_3, Y_train2_3, Y_val2_3 = train_test_split(X_train2_3,Y_train2_3,test_size=test_percentage,stratify=Y_train2_3, random_state=42)\n",
    "#     X_train3_4, X_test3_4, Y_train3_4, Y_test3_4 = train_test_split(value3_4, tags3_4, test_size=test_percentage,stratify=data3_4['gender'], random_state=42)\n",
    "#     X_train3_4, X_val3_4, Y_train3_4, Y_val3_4 = train_test_split(X_train3_4,Y_train3_4,test_size=test_percentage,stratify=Y_train3_4, random_state=42)\n",
    "    \n",
    "#     C_range=[0.01, 0.1, 1, 10, 100, 1000]\n",
    "#     gamma_range=[1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5]\n",
    "#     parameters= [\n",
    "#         {\n",
    "#             'kernel': ['rbf'],\n",
    "#             'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "#             'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "#         }#, \n",
    "#         #{\n",
    "#         #    'kernel': ['linear'],\n",
    "#         #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "#         #}, \n",
    "#         #{\n",
    "#         #    'kernel': ['sigmoid'],\n",
    "#         #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "#         #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "#         #}, \n",
    "#         #{\n",
    "#         #    'kernel': ['poly'],\n",
    "#         #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "#         #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "#         #}\n",
    "        \n",
    "#     ]\n",
    "#     # \n",
    "#     clf =GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "#     clf2_3 = GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "#     clf3_4 = GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "#     # pipe = Pipeline([('scaler', StandardScaler()), ('svm', clf)])\n",
    "#     clf.fit(X_train.values,Y_train)\n",
    "#     clf2_3.fit(X_train2_3.values,Y_train2_3)\n",
    "#     clf3_4.fit(X_train3_4.values,Y_train3_4)\n",
    "#     scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "#     scores2_3 = clf2_3.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "#     scores3_4 = clf3_4.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "#     print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "#     print(\"The best parameters are %s with a score of %0.2f\" % (clf2_3.best_params_, clf2_3.best_score_))\n",
    "#     print(\"The best parameters are %s with a score of %0.2f\" % (clf3_4.best_params_, clf3_4.best_score_))\n",
    "    \n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "#     plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "#     plt.xlabel('Gamma')\n",
    "#     plt.ylabel('C')\n",
    "#     plt.colorbar()\n",
    "#     plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "#     plt.yticks(np.arange(len(C_range)), C_range)\n",
    "#     fig=plt.title('Heat map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "#     fig.get_figure().savefig(base + r'Feature-Extraction/SVM/Heatmap-'+txt+'-'+str(int(test_percentage*100))+'%.jpg')\n",
    "#     plt.show()\n",
    "#     print(clf.best_params_)#mejor parametro\n",
    "\n",
    "    \n",
    "#     means = clf.cv_results_['mean_test_score']\n",
    "#     stds = clf.cv_results_['std_test_score']\n",
    "#     params = clf.cv_results_['params']\n",
    "#     for m, s, p in zip(means, stds, params):\n",
    "#         print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "#     y_pred = clf.predict(X_val)\n",
    "    \n",
    "    \n",
    "#     target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "#     target_names_2_3= [\"2\",\"3\"]\n",
    "#     target_names_3_4= [\"3\",\"4\"]\n",
    "    \n",
    "#     table=classification_report(Y_val,y_pred, target_names=target_names)\n",
    "#     table2_3=classification_report(Y_val2_3,clf2_3.predict(X_val2_3), target_names=target_names_2_3)\n",
    "#     table3_4=classification_report(Y_val3_4,clf3_4.predict(X_val3_4), target_names=target_names_3_4)\n",
    "#     table=str(table)\n",
    "#     table2_3=str(table2_3)\n",
    "#     table3_4=str(table3_4)              \n",
    "#     print(table)\n",
    "#     print(table2_3)\n",
    "#     print(table3_4)\n",
    "    \n",
    "#     mat=confusion_matrix(Y_val, y_pred)\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.xlabel('Actual')\n",
    "#     plt.ylabel('Predicted')\n",
    "#     Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "#                xticklabels=target_names, yticklabels= target_names )\n",
    "#     mat=plt.title('Confusion map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "#     Matrizconf.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-'+str(int(test_percentage*100))+'%.png')\n",
    "\n",
    "\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.xlabel('Actual')\n",
    "#     plt.ylabel('Predicted')\n",
    "#     mat2_3=confusion_matrix(Y_val2_3, clf2_3.predict(X_val2_3))\n",
    "#     Matrizconf2_3=sns.heatmap(mat2_3.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "#                 xticklabels=target_names_2_3, yticklabels= target_names_2_3 )\n",
    "    \n",
    "#     mat2_3=plt.title('Confusion map '+txt+'-2-3-'+str(int(test_percentage*100))+'%')\n",
    "#     Matrizconf2_3.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-2-3-'+str(int(test_percentage*100))+'%.png')\n",
    "\n",
    "    \n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.xlabel('Actual')\n",
    "#     plt.ylabel('Predicted')\n",
    "#     mat3_4=confusion_matrix(Y_val3_4, clf3_4.predict(X_val3_4))\n",
    "#     Matrizconf3_4=sns.heatmap(mat3_4.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "#                 xticklabels=target_names_3_4, yticklabels= target_names_3_4 ) \n",
    "#     mat3_4=plt.title('Confusion map '+txt+'-3-4-'+str(int(test_percentage*100))+'%')\n",
    "#     Matrizconf3_4.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-3-4-'+str(int(test_percentage*100))+'%.png')\n",
    "    \n",
    "    \n",
    "#     print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "#     print(Y_val.groupby(Y_val).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "#     joblib.dump(clf,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-'+str(int(test_percentage*100))+'%.pkl')\n",
    "#     joblib.dump(clf2_3,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-2-3-'+str(int(test_percentage*100))+'%.pkl')\n",
    "#     joblib.dump(clf3_4,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-3-4-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    \n",
    "#     # se llama el modelo\n",
    "#     #clf=joblib.load('modelo_entrenado.pkl')\n",
    "#     # se toma todo el dataset\n",
    "#     print(clf.score)\n",
    "#     print(\"Accuracy: \"+str(clf.score(X_test,Y_test)))\n",
    "#     print(\"Accuracy: \"+str(clf2_3.score(X_test2_3,Y_test2_3)))\n",
    "#     print(\"Accuracy: \"+str(clf3_4.score(X_test3_4,Y_test3_4)))\n",
    "\n",
    "#     return X_test, Y_test\n",
    "    \n",
    "# # porcentaje_test=[0.30,0.25,0.20]\n",
    "# # SVM(\"Elliptic-Fourier\",0.2)\n",
    "# # SVM(\"Geometric\" ,0.2)\n",
    "# x_test, y_test = SVM(\"Histogram-of-Oriented-Gradients-PCA\" ,0.2)\n",
    "# # SVM(\"Cof\" ,0.2)\n",
    "# # SVM(\"VHIST\" ,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract features from each photo in the directory image-segmentation-sara\n",
    "\n",
    "# test_data, test_value, test_tag = SVMutilities(\"Histogram-of-Oriented-Gradients-PCA\",0.2)\n",
    "# x_test = test_value\n",
    "# y_test = test_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 models\n",
    "clf = joblib.load('Feature-Extraction\\SVM\\modelo_entrenado-Histogram-of-Oriented-Gradients-PCA-20%.pkl')\n",
    "clf2_3 = joblib.load('Feature-Extraction\\SVM\\modelo_entrenado-Histogram-of-Oriented-Gradients-PCA-2-3-20%.pkl')\n",
    "clf3_4 = joblib.load('Feature-Extraction\\SVM\\modelo_entrenado-Histogram-of-Oriented-Gradients-PCA-3-4-20%.pkl')\n",
    "# predict the test data\n",
    "y_pred = clf.predict(x_test)\n",
    "# x_test2_3 = x_test[y_pred == '2' or y_pred == '3']\n",
    "# x_test3_4 = x_test[y_pred == '3' or y_pred == '4']\n",
    "y_pred2_3 = clf2_3.predict(x_test)\n",
    "y_pred3_4 = clf3_4.predict(x_test)\n",
    "y_predict_final = []\n",
    "\n",
    "# if the prediction is 2 or 3, then we use the second model to predict the final result\n",
    "# if the prediction is 3 or 4, then we use the third model to predict the final result\n",
    "# if the prediction is 0, 1, or 5, then we use the first model to predict the final result\n",
    "# if the prediction that out from the second and third models is 2 and 4 respectively, then we use the first model to predict the final result\n",
    "# if the prediction that out from the second and third models is 3 and 3 respectively, then we use the second model to predict the final result\n",
    "# if the prediction that out from the second and third models is 2 and 3 respectively, then we use the second model to predict the final result\n",
    "# if the prediction that out from the second and third models is 3 and 4 respectively, then we use the third model to predict the final result\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] == '0' or y_pred[i] == '1' or y_pred[i] == '5':\n",
    "        y_predict_final.append(y_pred[i])\n",
    "    elif (y_pred[i] == '2'):\n",
    "        y_predict_final.append(y_pred2_3[i])\n",
    "    elif (y_pred[i] == '4'):\n",
    "        y_predict_final.append(y_pred3_4[i])\n",
    "    elif (y_pred[i] == '3'):\n",
    "        if y_pred2_3[i] == '2':\n",
    "            y_predict_final.append(y_pred[i])\n",
    "        elif y_pred3_4[i] == '4':\n",
    "            y_predict_final.append(y_pred[i])\n",
    "        else:\n",
    "            y_predict_final.append(y_pred2_3[i])\n",
    "# # output the final result to a file with the y_test also\n",
    "# with open('Feature-Extraction\\SVM\\y_test.txt', 'w') as f:\n",
    "    # for item in y_test:\n",
    "        # f.write(\"%s\\n\" % item)\n",
    "# with open('Feature-Extraction\\SVM\\y_predict_final.txt', 'w') as f:\n",
    "    # for item in y_predict_final:\n",
    "        # f.write(\"%s\\n\" % item)\n",
    "        \n",
    "# calculate the accuracy manually\n",
    "count = 0\n",
    "count = sum(1 for i, j in zip(y_test, y_predict_final) if i == j)\n",
    "print(count/len(y_test) * 100)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "mat=confusion_matrix(y_test, y_predict_final)\n",
    "Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "           xticklabels=target_names, yticklabels= target_names )\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.xlabel('Actual')\n",
    "# plt.ylabel('Predicted')\n",
    "# mat=plt.title('Confusion map ')\n",
    "# Matrizconf.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-'+str(int(test_percentage*100))+'%.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"none\", n)\n",
    "# print(\"nont none\", nn)\n",
    "# print(\"none_w\", n_w)\n",
    "# print(\"nont none_w\", nn_w)\n",
    "# print(len(pca_features))\n",
    "# print( len(os.listdir(\"Image-Segmentation2/MEN\")))\n",
    "#pca_features = np.squeeze(np.array(pca_features[:-1]), axis = 2) # remove the third dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = np.array(pca_features)\n",
    "# temp = np.squeeze(temp)\n",
    "# print(temp[0])\n",
    "#array = pca_features.flat(1)\n",
    "# instead of pca_features to be array of arrays, we want it to be a 2d array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pca_features[0])\n",
    "# print(pca_features.shape)\n",
    "\n",
    "#pca_features = pca_features[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# accuracy = 0\n",
    "# model = SVC(kernel='rbf', gamma=0.1, C=10)\n",
    "# labels_encoded = LabelEncoder().fit_transform(labels)\n",
    "# # partition the data into training and testing splits, using 75%\n",
    "# trainingData, testData, trainingLabels, testLabels = train_test_split(pca_features, labels_encoded, test_size=0.2, random_state=42)\n",
    "# trainingData, validationData, trainingLabels, validationLabels = train_test_split(trainingData, trainingLabels, test_size=0.2, random_state=42)\n",
    "# trainingLabels = np.reshape(trainingLabels, trainingLabels.shape[0])\n",
    "\n",
    "# def train_model_hog():\n",
    "    \n",
    "#     #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "#      # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "\n",
    "    \n",
    "\n",
    "#     print(\"[INFO] evaluating...\", flush=True)\n",
    "    \n",
    "#     # train the model\n",
    "#     # how to continue to train SVM based on the previous model\n",
    "#     # this is done by using the partial_fit method\n",
    "#     # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "    \n",
    "    \n",
    "#     # trainingData = trainingData.reshape(-1, 1)\n",
    "#     # trainingLabels = trainingLabels.reshape(-1, 1)\n",
    "#     # now padding the data to be 2D array\n",
    "    \n",
    "#     #print(trainingLabels)\n",
    "#     model.fit(trainingData, trainingLabels)\n",
    "\n",
    "#     # evaluate the model and update the accuracies list\n",
    "#     acc = model.score(validationData, validationLabels)\n",
    "#     accuracy = acc\n",
    "#     print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "#     # dump the classifier to file\n",
    "#     print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "#     f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "#     f.write(pickle.dumps(model))\n",
    "#     f.close()\n",
    "\n",
    "\n",
    "# train_model_hog()\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model and update the accuracies list\n",
    "# acc = model.score(testData, testLabels)\n",
    "# accuracy = acc\n",
    "# print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "# # dump the classifier to file\n",
    "# print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "# f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "# f.write(pickle.dumps(model))\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# def train_model_hog():\n",
    "#     k = 0\n",
    "#     #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "#     model = SVC(kernel='rbf', gamma=0.01, C=10) # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "  \n",
    "#     labels_data = pd.read_csv(\"HOG_PCA_FEATURES.csv\", header=None, sep=',')\n",
    "\n",
    "#     data_temp = np.array(labels_data)\n",
    "#     for j in range (len(labels_data)):\n",
    "#         temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "#         temp = temp.replace(\"\\n\", \"\")\n",
    "#         temp = temp.replace(\"]\", \"\")\n",
    "#         temp= temp.split(\" \")\n",
    "#         temp = np.array(temp)\n",
    "#         if(temp.__contains__('')):\n",
    "#             temp = np.delete(temp, np.where(temp == ''))\n",
    "#         data.append(np.array(temp, dtype=np.float32))\n",
    "#         labels.append(labels_data.iloc[j, 0])\n",
    "\n",
    "#     labels_data = pd.read_csv(\"HOG_PCA_FEATURES.csv\", header=None, sep=',')\n",
    "\n",
    "#     data_temp = np.array(labels_data)\n",
    "#     for j in range (len(labels_data)):\n",
    "#         temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "#         temp = temp.replace(\"\\n\", \"\")\n",
    "#         temp = temp.replace(\"]\", \"\")\n",
    "#         temp= temp.split(\" \")\n",
    "#         temp = np.array(temp)\n",
    "#         if(temp.__contains__('')):\n",
    "#             temp = np.delete(temp, np.where(temp == ''))\n",
    "#         #print(data_temp)\n",
    "#         data.append(np.array(temp, dtype=np.float32))\n",
    "#         labels.append(labels_data.iloc[j, 0])\n",
    "    \n",
    "#         data = np.array(data, dtype=\"float\")\n",
    "#         labels = LabelEncoder().fit_transform(labels)\n",
    "#         # partition the data into training and testing splits, using 75%\n",
    "#         trainingData, testData, trainingLabels, testLabels = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "        \n",
    "\n",
    "#         print(\"[INFO] evaluating...\", flush=True)\n",
    "        \n",
    "#         # train the model\n",
    "#         # how to continue to train SVM based on the previous model\n",
    "#         # this is done by using the partial_fit method\n",
    "#         # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "#         model.fit(trainingData, trainingLabels)\n",
    "\n",
    "#         # evaluate the model and update the accuracies list\n",
    "#         acc = model.score(testData, testLabels)\n",
    "#         print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "#         # dump the classifier to file\n",
    "#         print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "#         f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "#         f.write(pickle.dumps(model))\n",
    "#         f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
