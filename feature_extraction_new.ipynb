{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# the imutils package is a collection of convenience functions\n",
    "# to make basic image processing functions such as translation, rotation, resizing, skeletonization, and displaying Matplotlib images easier with OpenCV and both Python 2.7 and Python 3\n",
    "import skimage.io as io\n",
    "from imutils import paths\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage.feature import hog\n",
    "import numpy as np\n",
    "from skimage import exposure\n",
    "from skimage import feature\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "# for io.imshow we need to import it from skimage\n",
    "# the syntax is import skimage.io as io\n",
    "# linearsvc is a linear support vector machine and needs to be imported from sklearn.svm\n",
    "# the syntax is from sklearn.svm import LinearSVC\n",
    "# labelencoder is a label encoder and needs to be imported from sklearn.preprocessing\n",
    "# the syntax is from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "# import sklearn.svm.SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# for pickle we need to import it from sklearn.externals\n",
    "# the syntax for pickle is import pickle\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use hog implemented in skimage.feature\n",
    "\n",
    "def get_hog():\n",
    "    # get images from images array, convert to grayscale\n",
    "    # and resize to 200x200 \n",
    "\n",
    "    # try:\n",
    "    #     os.remove(\"HOG_PCA_FEATURES.csv\")\n",
    "    # except OSError:\n",
    "    #     pass  \n",
    "\n",
    "    # features_file = open(\"HOG_PCA_FEATURES.csv\", \"w\", newline='')\n",
    "    labels = []\n",
    "    pca_features = []\n",
    "\n",
    "    zeft = \"grey_mo3eed\"\n",
    "    for imagePath in os.listdir(zeft):\n",
    "        # extract the label from the image path\n",
    "        # we can get the label from the image name as the label is the first char of the image name\n",
    "        # for example, the label of image 1_1.jpg is 1\n",
    "        labels.append((int)(imagePath.split(os.path.sep)[-1][0]))\n",
    "        \n",
    "        # load the image, convert it to grayscale, and detect\n",
    "        # edges in it\n",
    "        image = cv2.imread(zeft+\"/\" + imagePath)\n",
    "        \n",
    "        # extract Histogram of Oriented Gradients from the\n",
    "        # test image\n",
    "        # display the original image\n",
    "        # if the image exists, display it and count the number of images\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image = cv2.resize(image, (461, 260))\n",
    "            (H, hogImage) = feature.hog(image, orientations=9,  pixels_per_cell=(16, 16), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "            # H = H.reshape(-1, 1)\n",
    "            # pca = PCA(0.8).fit(H)\n",
    "            # components = pca.transform(H)\n",
    "            pca_features.append(H)\n",
    "\n",
    "            \n",
    "\n",
    "            # store the label and features in the csv file\n",
    "            #csv.writer(features_file).writerow([labels[-1], pca_features])\n",
    "\n",
    "    return pca_features, np.array(labels)        \n",
    "\n",
    "\n",
    "pca_features, labels = get_hog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"none\", n)\n",
    "# print(\"nont none\", nn)\n",
    "# print(\"none_w\", n_w)\n",
    "# print(\"nont none_w\", nn_w)\n",
    "# print(len(pca_features))\n",
    "# print( len(os.listdir(\"Image-Segmentation2/MEN\")))\n",
    "#pca_features = np.squeeze(np.array(pca_features[:-1]), axis = 2) # remove the third dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = np.array(pca_features)\n",
    "# temp = np.squeeze(temp)\n",
    "# print(temp[0])\n",
    "#array = pca_features.flat(1)\n",
    "# instead of pca_features to be array of arrays, we want it to be a 2d array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pca_features[0])\n",
    "# print(pca_features.shape)\n",
    "\n",
    "#pca_features = pca_features[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "[INFO] evaluating...\n",
      "[INFO] accuracy: 60.71%\n",
      "[INFO] dumping classifier to file...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "accuracy = 0\n",
    "model = SVC(kernel='rbf', gamma=0.1, C=10)\n",
    "labels_encoded = LabelEncoder().fit_transform(labels)\n",
    "# partition the data into training and testing splits, using 75%\n",
    "trainingData, testData, trainingLabels, testLabels = train_test_split(pca_features, labels_encoded, test_size=0.1, random_state=42)\n",
    "trainingData, validationData, trainingLabels, validationLabels = train_test_split(trainingData, trainingLabels, test_size=0.1, random_state=42)\n",
    "trainingLabels = np.reshape(trainingLabels, trainingLabels.shape[0])\n",
    "print(len(trainingLabels))\n",
    "def train_model_hog():\n",
    "    \n",
    "    #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "     # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"[INFO] evaluating...\", flush=True)\n",
    "    \n",
    "    # train the model\n",
    "    # how to continue to train SVM based on the previous model\n",
    "    # this is done by using the partial_fit method\n",
    "    # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "    \n",
    "    \n",
    "    # trainingData = trainingData.reshape(-1, 1)\n",
    "    # trainingLabels = trainingLabels.reshape(-1, 1)\n",
    "    # now padding the data to be 2D array\n",
    "    \n",
    "    #print(trainingLabels)\n",
    "    model.fit(trainingData, trainingLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    acc = model.score(validationData, validationLabels)\n",
    "    accuracy = acc\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "    # dump the classifier to file\n",
    "    print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "    f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "    f.write(pickle.dumps(model))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "train_model_hog()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "[INFO] evaluating...\n",
      "[INFO] accuracy: 64.29%\n",
      "[INFO] dumping classifier to file...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "accuracy2 = 0\n",
    "# model = SVC(kernel='rbf', gamma=0.1, C=10)\n",
    "rf = RandomForestClassifier(n_estimators=2500)\n",
    "# labels_encoded = LabelEncoder().fit_transform(labels)\n",
    "# # partition the data into training and testing splits, using 75%\n",
    "# trainingData, testData, trainingLabels, testLabels = train_test_split(pca_features, labels_encoded, test_size=0.2, random_state=42)\n",
    "# trainingData, validationData, trainingLabels, validationLabels = train_test_split(trainingData, trainingLabels, test_size=0.2, random_state=42)\n",
    "# trainingLabels = np.reshape(trainingLabels, trainingLabels.shape[0])\n",
    "print(len(trainingLabels))\n",
    "def train_model_rf():\n",
    "    \n",
    "    #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "     # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"[INFO] evaluating...\", flush=True)\n",
    "    \n",
    "    # train the model\n",
    "    # how to continue to train SVM based on the previous model\n",
    "    # this is done by using the partial_fit method\n",
    "    # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "    \n",
    "    \n",
    "    # trainingData = trainingData.reshape(-1, 1)\n",
    "    # trainingLabels = trainingLabels.reshape(-1, 1)\n",
    "    # now padding the data to be 2D array\n",
    "    \n",
    "    #print(trainingLabels)\n",
    "    rf.fit(trainingData, trainingLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    acc = rf.score(validationData, validationLabels)\n",
    "    accuracy2 = acc\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "    # dump the classifier to file\n",
    "    print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "    f = open(\"rf.pkl\", \"wb\") # wb = write binary\n",
    "    f.write(pickle.dumps(rf))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "train_model_rf()\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "[INFO] evaluating...\n",
      "[INFO] accuracy: 68.88%\n",
      "[INFO] dumping classifier to file...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "accuracy3 = 0\n",
    "# labels_encoded = LabelEncoder().fit_transform(labels)\n",
    "# # partition the data into training and testing splits, using 75%\n",
    "# trainingData, testData, trainingLabels, testLabels = train_test_split(pca_features, labels_encoded, test_size=0.2, random_state=42)\n",
    "# trainingData, validationData, trainingLabels, validationLabels = train_test_split(trainingData, trainingLabels, test_size=0.2, random_state=42)\n",
    "# trainingLabels = np.reshape(trainingLabels, trainingLabels.shape[0])\n",
    "print(len(trainingLabels))\n",
    "def train_model_nb():\n",
    "    \n",
    "    #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "     # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"[INFO] evaluating...\", flush=True)\n",
    "    \n",
    "    # train the model\n",
    "    # how to continue to train SVM based on the previous model\n",
    "    # this is done by using the partial_fit method\n",
    "    # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "    \n",
    "    \n",
    "    # trainingData = trainingData.reshape(-1, 1)\n",
    "    # trainingLabels = trainingLabels.reshape(-1, 1)\n",
    "    # now padding the data to be 2D array\n",
    "    \n",
    "    #print(trainingLabels)\n",
    "    nb.fit(trainingData, trainingLabels)\n",
    "\n",
    "    # evaluate the model and update the accuracies list\n",
    "    acc = nb.score(validationData, validationLabels)\n",
    "    accuracy3 = acc\n",
    "    print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "    # dump the classifier to file\n",
    "    print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "    f = open(\"nb.pkl\", \"wb\") # wb = write binary\n",
    "    f.write(pickle.dumps(nb))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "train_model_nb()\n",
    "print(accuracy3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] accuracy: 60.09%\n",
      "[INFO] dumping classifier to file...\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model and update the accuracies list\n",
    "acc = model.score(testData, testLabels)\n",
    "accuracy = acc\n",
    "print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "# dump the classifier to file\n",
    "print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "# f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "# f.write(pickle.dumps(model))\n",
    "# f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RF] accuracy: 72.48%\n"
     ]
    }
   ],
   "source": [
    "acc2 = rf.score(testData, testLabels)\n",
    "accuracy2 = acc2\n",
    "print(\"[RF] accuracy: {:.2f}%\".format(acc2 * 100), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RF] accuracy: 63.30%\n"
     ]
    }
   ],
   "source": [
    "acc3 = nb.score(testData, testLabels)\n",
    "accuracy3 = acc3\n",
    "print(\"[RF] accuracy: {:.2f}%\".format(acc3 * 100), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /content/kaggle/dataset/dataset\n",
    "def plt_t(title, img, cmap=None):\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  0.2897196261682243\n",
      "correct rf:  0.3925233644859813\n",
      "correct rf:  0.32710280373831774\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load model and predict class\n",
    "# clf = joblib.load(r'Feature-Extraction\\SVM\\modelo_entrenado-Histogram-of-Oriented-Gradients-PCA-20%.pkl')\n",
    "# clf.predict(value)\n",
    "\n",
    "# loop over the testing images in us folder\n",
    "\n",
    "path = r\"./data we7sha\"\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "rf_correct = 0\n",
    "rf_total = 0\n",
    "\n",
    "nb_correct = 0\n",
    "nb_total = 0\n",
    "\n",
    "avg_correct = 0\n",
    "avg_total = 0\n",
    "\n",
    "for (path, _, archivos) in walk(path):\n",
    "    for arch in archivos:\n",
    "        \n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                # load the image, convert it to grayscale, and resize it to be a fixed\n",
    "                # 64x64 pixels, ignoring aspect ratio\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                # print(direc)\n",
    "                # image = preprocess(direc)\n",
    "                # plt_t(\"after preprocess\",image, cmap='gray')\n",
    "                # image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "                image = cv2.imread(direc,0)\n",
    "                # image = cv2.cvtColor(image, cv2.COLOR)\n",
    "                \n",
    "                (value,_) = feature.hog(image, orientations=9,  pixels_per_cell=(16, 16), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "                # value = combineFeatures(image)\n",
    "                # load the image and classify it\n",
    "                # print(path[-1])\n",
    "                value = value.reshape(1, -1)\n",
    "                # print(nomArch[0])\n",
    "                # print(model.predict(value)[0])\n",
    "                is_corr = 1 if (model.predict(value)[0]) == int(nomArch[0]) else 0\n",
    "                # if(is_corr == 0):\n",
    "                    # print(\"incorrect: \",direc)\n",
    "                    # plt_t(\"incorrect\",image, cmap='gray')\n",
    "                \n",
    "                correct += is_corr\n",
    "                total+=1\n",
    "                \n",
    "                is_corr2 = 1 if (rf.predict(value)[0]) == int(nomArch[0]) else 0\n",
    "                rf_correct += is_corr2\n",
    "                rf_total+=1\n",
    "                \n",
    "                is_corr3 = 1 if (nb.predict(value)[0]) == int(nomArch[0]) else 0\n",
    "                nb_correct += is_corr3\n",
    "                nb_total+=1\n",
    "                \n",
    "                \n",
    "                # show the prediction\n",
    "print(\"correct: \",correct/total)\n",
    "print(\"correct rf: \",rf_correct/rf_total)\n",
    "print(\"correct rf: \",nb_correct/nb_total)\n",
    "# print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train the model\n",
    "# def train_model_hog():\n",
    "#     k = 0\n",
    "#     #model = SVC(random_state=42) # ramdom state is used to get the same results each time. It means that the model will be trained on the same data each time\n",
    "#     model = SVC(kernel='rbf', gamma=0.01, C=10) # rbf means that the model will use the gaussian kernel as it stands for radial basis function\n",
    "  \n",
    "#     labels_data = pd.read_csv(\"HOG_PCA_FEATURES.csv\", header=None, sep=',')\n",
    "\n",
    "#     data_temp = np.array(labels_data)\n",
    "#     for j in range (len(labels_data)):\n",
    "#         temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "#         temp = temp.replace(\"\\n\", \"\")\n",
    "#         temp = temp.replace(\"]\", \"\")\n",
    "#         temp= temp.split(\" \")\n",
    "#         temp = np.array(temp)\n",
    "#         if(temp.__contains__('')):\n",
    "#             temp = np.delete(temp, np.where(temp == ''))\n",
    "#         data.append(np.array(temp, dtype=np.float32))\n",
    "#         labels.append(labels_data.iloc[j, 0])\n",
    "\n",
    "#     labels_data = pd.read_csv(\"HOG_PCA_FEATURES.csv\", header=None, sep=',')\n",
    "\n",
    "#     data_temp = np.array(labels_data)\n",
    "#     for j in range (len(labels_data)):\n",
    "#         temp = data_temp[j, 1].replace(\"[\", \"\")\n",
    "#         temp = temp.replace(\"\\n\", \"\")\n",
    "#         temp = temp.replace(\"]\", \"\")\n",
    "#         temp= temp.split(\" \")\n",
    "#         temp = np.array(temp)\n",
    "#         if(temp.__contains__('')):\n",
    "#             temp = np.delete(temp, np.where(temp == ''))\n",
    "#         #print(data_temp)\n",
    "#         data.append(np.array(temp, dtype=np.float32))\n",
    "#         labels.append(labels_data.iloc[j, 0])\n",
    "    \n",
    "#         data = np.array(data, dtype=\"float\")\n",
    "#         labels = LabelEncoder().fit_transform(labels)\n",
    "#         # partition the data into training and testing splits, using 75%\n",
    "#         trainingData, testData, trainingLabels, testLabels = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "        \n",
    "\n",
    "#         print(\"[INFO] evaluating...\", flush=True)\n",
    "        \n",
    "#         # train the model\n",
    "#         # how to continue to train SVM based on the previous model\n",
    "#         # this is done by using the partial_fit method\n",
    "#         # syntax: model.partial_fit(trainingData, trainingLabels)\n",
    "#         model.fit(trainingData, trainingLabels)\n",
    "\n",
    "#         # evaluate the model and update the accuracies list\n",
    "#         acc = model.score(testData, testLabels)\n",
    "#         print(\"[INFO] accuracy: {:.2f}%\".format(acc * 100), flush=True)\n",
    "#         # dump the classifier to file\n",
    "#         print(\"[INFO] dumping classifier to file...\", flush=True)\n",
    "#         f = open(\"SVM_classifier.pkl\", \"wb\") # wb = write binary\n",
    "#         f.write(pickle.dumps(model))\n",
    "#         f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
