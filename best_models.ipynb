{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "88TYUAYQQRs2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "This experiment developed a system that is designed to facilitate communication between people who have vocal-auditory disability. The experiment has machine learning techniques to perform the due process of recognition of hand gestures of the Colombian sign language, recognizing the numbers from 0 to 5 and the vowels. \n",
    "\n",
    "This experiment works through 4 stages: taking photographs, pre-processing the photo, extracting the characteristics of the photo and finally performs the classification process for the identification of the gesture being carried out. \n",
    "The image is captured by any camera that has a good quality shot. Then move on to the next stage of pre-processing, where you want to clean the techniques to remove the shadow, the background and leave the image clean to perform the process of segmentation where the process of eliminating the noises that this pose takes place. \n",
    "\n",
    "At the stage of extraction of characteristics, it extracts the characteristics of the image that give us the mathematical methods like: Moments of Hu, ellipticals of Fourier, histograms oriented to gradients (HOG) and geometric characteristics. \n",
    "Finally, using the vector support machine classifier (SVM) you get the value of the sign, if it is a number or a vowel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j-nvfI7qQRs4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Exploratory Analysis\n",
    "To begin this exploratory analysis, first import libraries and define functions perform pre-processing and extracting features ` Cv2` (OpenCV), ` Sklearn`, `scipy`, `skimage`. \n",
    "Use the `pyefd` library to get the Fourier ellipticals.\n",
    "For the classification processes the library of `sklearn` was used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeglP3nmQhfw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['KAGGLE_CONFIG_DIR'] = \"/content/kaggle\"\n",
    "# !kaggle datasets download -d evernext10/hand-gesture-of-the-colombian-sign-language\n",
    "# !unzip '/content/kaggle/hand-gesture-of-the-colombian-sign-language.zip' -d '/content/kaggle/dataset'\n",
    "# !rm -rf /content/kaggle/dataset/men/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xH-H6nVQRs4",
    "outputId": "f45bd6e5-c1d6-4f58-9193-355e9b296e0e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install pyefd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AbsJ6-jvQRs5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import cv2 as cv2\n",
    "# import cv2.cv2 as cv2\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "import imageio\n",
    "from os import walk\n",
    "from pyefd import elliptic_fourier_descriptors\n",
    "from skimage import feature\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV  \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns;\n",
    "import collections\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import skew\n",
    "from skimage import segmentation\n",
    "from skimage.filters import sobel\n",
    "sns.set()\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SURMarqrQRs5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is 0 csv file in the current version of the dataset:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MeDg1lXWQRs5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO9FsGYxpCHt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf /content/kaggle/dataset/dataset\n",
    "def plt_t(title, img, cmap=None):\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5wdYM_99QRs6",
    "outputId": "dfe254c4-3a89-43f4-c267-456c6ad49c3a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Pre-processing of the images is done\n",
    "base = './'\n",
    "def segm_1(img_rgb):\n",
    "  img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2YCR_CB)  \n",
    "  ycrcbmin = np.array((0, 133, 77))\n",
    "  ycrcbmax = np.array((255, 173, 127))\n",
    "  skin_ycrcb = cv2.inRange(img_ycrcb, ycrcbmin, ycrcbmax)\n",
    "  kernel = np.ones((5, 5), np.uint8)\n",
    "  img_erode = cv2.erode(skin_ycrcb, kernel, iterations=2)  \n",
    "  holesimg = ndi.binary_fill_holes(img_erode).astype(np.int) \n",
    "  return holesimg\n",
    "\n",
    "def segm_2(img_rgb):\n",
    "  lab = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2LAB)\n",
    "  # bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  _, _, r = cv2.split(lab)\n",
    "  r = cv2.normalize(r, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "  elevation_map = sobel(r)\n",
    "  markers = np.zeros_like(r)\n",
    "  markers[r < 30] = 1\n",
    "  markers[r > 150] = 2\n",
    "  segmentation_coins = cv2.watershed(img_rgb, markers)\n",
    "  return segmentation_coins\n",
    "\n",
    "\n",
    "def segm_3(img_rgb):\n",
    "  blur = cv2.GaussianBlur(img_rgb, (3, 3), 0)\n",
    "\n",
    "  # Convert image to LAB color space\n",
    "  lab = cv2.cvtColor(blur, cv2.COLOR_BGR2LAB)\n",
    "  # bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  # lab = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2LAB)\n",
    "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "  lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "\n",
    "  # Convert image back to BGR color space\n",
    "  bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  # it was 50, 190\n",
    "  canny = cv2.Canny(bgr, 10, 170)\n",
    "  # opening = cv2.morphologyEx(canny, cv2.MORPH_OPEN, (3, 3))\n",
    "  return canny\n",
    "\n",
    "def is_lightened(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    quantiles = np.quantile(v, [0.5])\n",
    "    # print(quantiles)\n",
    "    return quantiles[0] > 230\n",
    "\n",
    "def segm_4(img):\n",
    "    # Convert image to LAB color space\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    # Apply CLAHE to enhance contrast in LAB lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(18,18))\n",
    "    lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "\n",
    "    # Convert image back to BGR color space\n",
    "    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    # Display original and preprocessed images side by side\n",
    "    # apply cannny edge detection\n",
    "    edges = cv2.Canny(bgr, 100, 200)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 8))\n",
    "    opening = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    from skimage import morphology\n",
    "    holesimg = ndi.binary_fill_holes(opening).astype(np.int8)\n",
    "    bin = holesimg>0.5\n",
    "    a = morphology.remove_small_objects(bin, 1000)\n",
    "    return (a*255).astype(np.uint8)\n",
    "\n",
    "def segm_5(img):\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mk = img_hsv > np.array([0, 0, 230])\n",
    "    mk = mk.astype(np.float32)\n",
    "    mask = (mk *0.5 + 0.5)\n",
    "    masked = (mask* img_hsv)\n",
    "    img_partly_darken = cv2.cvtColor(masked, cv2.COLOR_HSV2RGB)\n",
    "    green_mask = (img_partly_darken[:, :, 0] > img_partly_darken[:, :, 1]).astype(np.float32)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    img_erode = cv2.erode(green_mask, kernel, iterations=2)  \n",
    "    holesimg = ndi.binary_fill_holes(img_erode).astype(np.int) \n",
    "    return holesimg\n",
    "  \n",
    "def segm_6(img):\n",
    "#     blur the img then turn it into gray scale\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    gray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def segm_7(img):\n",
    "    # Convert BGR to HSV color space\n",
    "    hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n",
    "    # Split HSV image into separate channels\n",
    "    _, _, B_channel = cv2.split(hsv_img)\n",
    "    _, trr2 = cv2.threshold(\n",
    "        B_channel, 1, 1.0, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    \n",
    "    trr2 = ndi.binary_fill_holes(trr2).astype(np.int8)\n",
    "    masked_data = cv2.bitwise_and(img, img, mask=trr2)\n",
    "    return cv2.cvtColor(masked_data, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "  \n",
    "def make_to_left(img):\n",
    "  a = img.sum(axis=0)\n",
    "  a = a / a.max()\n",
    "  if(a[:int(len(a)//8)].sum() < a[int(len(a)*7//8):].sum()):\n",
    "    # flip the image\n",
    "    return cv2.flip(img, 1)\n",
    "  return img\n",
    "\n",
    "def preprocess(direc):\n",
    "  try:\n",
    "    img_rgb = cv2.imread(direc)\n",
    "    img_rgb = cv2.resize(img_rgb, (461, 260))\n",
    "  except:\n",
    "    print(\"cant read image\")\n",
    "    return None\n",
    "  # if(is_lightened(img_rgb)):\n",
    "  #   holesimg = segm_5(img_rgb)\n",
    "  # else:\n",
    "  #   holesimg = segm_1(img_rgb)\n",
    "  holesimg = segm_7(img_rgb)\n",
    "  # holesimg = segm_4(img_rgb)\n",
    "  # holesimg = segm_6(img_rgb)\n",
    "  holesimg = make_to_left(holesimg)\n",
    "  return holesimg\n",
    "\n",
    "\n",
    "def ImageSegmentation():\n",
    "    path_IS = r\"./Image-Segmentation2\"\n",
    "    if not os.path.exists(path_IS):\n",
    "        os.makedirs(path_IS)\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./dataset\"\n",
    "\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        # print(path)\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                # label = path.split(\"\\\\\")[-1]\n",
    "                # print(label)\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name =  nomArch + ext\n",
    "                # print(path + \"/\" + nomArch + ext)\n",
    "                \n",
    "                holesimg = preprocess(direc)\n",
    "                if(holesimg is None):\n",
    "                  continue\n",
    "                # holesimage = segm_3(holesimg)\n",
    "                imageio.imwrite(os.path.join(path_IS, name), holesimg)\n",
    "                \n",
    "ImageSegmentation()\n",
    "# plt.show()\n",
    "# img = cv2.imread(r\"dataset\\men\\1\\1_men (1).JPG\")\n",
    "# img_p = preprocess(r\"dataset\\men\\1\\1_men (2).JPG\")\n",
    "# plt_t('',img_p,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Pre-processing of the images is done\n",
    "base = './'\n",
    "def segm_1(img_rgb):\n",
    "  img_ycrcb = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2YCR_CB)  \n",
    "  ycrcbmin = np.array((0, 133, 77))\n",
    "  ycrcbmax = np.array((255, 173, 127))\n",
    "  skin_ycrcb = cv2.inRange(img_ycrcb, ycrcbmin, ycrcbmax)\n",
    "  kernel = np.ones((5, 5), np.uint8)\n",
    "  img_erode = cv2.erode(skin_ycrcb, kernel, iterations=2)  \n",
    "  holesimg = ndi.binary_fill_holes(img_erode).astype(np.int) \n",
    "  return holesimg\n",
    "\n",
    "def segm_2(img_rgb):\n",
    "  lab = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2LAB)\n",
    "  # bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  _, _, r = cv2.split(lab)\n",
    "  r = cv2.normalize(r, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n",
    "  elevation_map = sobel(r)\n",
    "  markers = np.zeros_like(r)\n",
    "  markers[r < 30] = 1\n",
    "  markers[r > 150] = 2\n",
    "  segmentation_coins = cv2.watershed(img_rgb, markers)\n",
    "  return segmentation_coins\n",
    "\n",
    "\n",
    "def segm_3(img_rgb):\n",
    "  blur = cv2.GaussianBlur(img_rgb, (3, 3), 0)\n",
    "\n",
    "  # Convert image to LAB color space\n",
    "  lab = cv2.cvtColor(blur, cv2.COLOR_BGR2LAB)\n",
    "  # bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  # lab = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2LAB)\n",
    "  clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
    "  lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "\n",
    "  # Convert image back to BGR color space\n",
    "  bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "  # it was 50, 190\n",
    "  canny = cv2.Canny(bgr, 10, 170)\n",
    "  # opening = cv2.morphologyEx(canny, cv2.MORPH_OPEN, (3, 3))\n",
    "  return canny\n",
    "\n",
    "def is_lightened(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    quantiles = np.quantile(v, [0.5])\n",
    "    # print(quantiles)\n",
    "    return quantiles[0] > 230\n",
    "\n",
    "def segm_4(img):\n",
    "    # Convert image to LAB color space\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    # Apply CLAHE to enhance contrast in LAB lightness channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=10.0, tileGridSize=(18,18))\n",
    "    lab[:, :, 0] = clahe.apply(lab[:, :, 0])\n",
    "\n",
    "    # Convert image back to BGR color space\n",
    "    bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "    # Display original and preprocessed images side by side\n",
    "    # apply cannny edge detection\n",
    "    edges = cv2.Canny(bgr, 100, 200)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (1, 8))\n",
    "    opening = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    from skimage import morphology\n",
    "    holesimg = ndi.binary_fill_holes(opening).astype(np.int8)\n",
    "    bin = holesimg>0.5\n",
    "    a = morphology.remove_small_objects(bin, 1000)\n",
    "    return (a*255).astype(np.uint8)\n",
    "\n",
    "def segm_5(img):\n",
    "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    mk = img_hsv > np.array([0, 0, 230])\n",
    "    mk = mk.astype(np.float32)\n",
    "    mask = (mk *0.5 + 0.5)\n",
    "    masked = (mask* img_hsv)\n",
    "    img_partly_darken = cv2.cvtColor(masked, cv2.COLOR_HSV2RGB)\n",
    "    green_mask = (img_partly_darken[:, :, 0] > img_partly_darken[:, :, 1]).astype(np.float32)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    img_erode = cv2.erode(green_mask, kernel, iterations=2)  \n",
    "    holesimg = ndi.binary_fill_holes(img_erode).astype(np.int) \n",
    "    return holesimg\n",
    "  \n",
    "def segm_6(img):\n",
    "#     blur the img then turn it into gray scale\n",
    "    blur = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    gray = cv2.cvtColor(blur, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def make_to_left(img):\n",
    "  a = img.sum(axis=0)\n",
    "  a = a / a.max()\n",
    "  if(a[:int(len(a)//8)].sum() < a[int(len(a)*7//8):].sum()):\n",
    "    # flip the image\n",
    "    return cv2.flip(img, 1)\n",
    "  return img\n",
    "\n",
    "def preprocess(direc):\n",
    "  try:\n",
    "    img_rgb = cv2.imread(direc)\n",
    "    img_rgb = cv2.resize(img_rgb, (461, 260))\n",
    "  except:\n",
    "    print(\"cant read image\")\n",
    "    return None\n",
    "  # if(is_lightened(img_rgb)):\n",
    "  #   holesimg = segm_5(img_rgb)\n",
    "  # else:\n",
    "  #   holesimg = segm_1(img_rgb)\n",
    "  holesimg = segm_3(img_rgb)\n",
    "  # holesimg = segm_4(img_rgb)\n",
    "  # holesimg = segm_6(img_rgb)\n",
    "  holesimg = make_to_left(holesimg)\n",
    "  return holesimg\n",
    "\n",
    "\n",
    "def ImageSegmentation():\n",
    "    path_IS = r\"./Image-Segmentation2\"\n",
    "    if not os.path.exists(path_IS):\n",
    "        os.makedirs(path_IS)\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./dataset\"\n",
    "\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        # print(path)\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                label = path.split(\"\\\\\")[-1]\n",
    "                # print(label)\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name =  label + \"_\" + nomArch + ext\n",
    "                # print(path + \"/\" + nomArch + ext)\n",
    "                \n",
    "                holesimg = preprocess(direc)\n",
    "                if(holesimg is None):\n",
    "                  continue\n",
    "                # holesimage = segm_3(holesimg)\n",
    "                imageio.imwrite(os.path.join(path_IS, name), holesimg)\n",
    "                \n",
    "ImageSegmentation()\n",
    "# plt.show()\n",
    "# img = cv2.imread(r\"dataset\\men\\1\\1_men (1).JPG\")\n",
    "# img_p = preprocess(r\"dataset\\men\\1\\1_men (2).JPG\")\n",
    "# plt_t('',img_p,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extratct sift key points and descriptors and draw them\n",
    "def extract_sift(img):\n",
    "  # sift = cv2.xfeatures2d.SIFT_create()\n",
    "  orb = cv2.ORB_create()\n",
    "\n",
    "  kp, des = orb.detectAndCompute(img, None)\n",
    "  return kp, des\n",
    "\n",
    "def draw_sift(img, kp):\n",
    "    img = cv2.drawKeypoints(img, kp, None)\n",
    "    return img\n",
    "\n",
    "img = cv2.imread(r\"Image-Segmentation2/0_men (3).JPG\")\n",
    "img = cv2.resize(img, (461, 260))\n",
    "kp, des = extract_sift(img)\n",
    "img = draw_sift(img, kp)\n",
    "plt.imshow(img)\n",
    "\n",
    "\n",
    "\n",
    "base = \"./\"\n",
    "def CountFlips():\n",
    "    print(\"CountFlips\\n\")\n",
    "    path_EF = base + r\"Feature-Extraction\"\n",
    "    if not os.path.exists(path_EF):\n",
    "        os.makedirs(path_EF)\n",
    "\n",
    "    file = open(base + r\"Feature-Extraction/CountFlips.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    all_lings = []\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                kp, des = extract_sift(img_binary)\n",
    "                all_lings.append(len(kp))\n",
    "\n",
    "    all_lings = np.array(all_lings)\n",
    "    print(all_lings.mean())\n",
    "    print(all_lings.std())\n",
    "    print(all_lings.max())\n",
    "    print(all_lings.min())\n",
    "CountFlips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"./\"\n",
    "def VHist():\n",
    "    print(\"VHIST\\n\")\n",
    "    path_EF = base + r\"Feature-Extraction\"\n",
    "    if not os.path.exists(path_EF):\n",
    "        os.makedirs(path_EF)\n",
    "\n",
    "    file = open(base + r\"Feature-Extraction/VHIST.txt\", \"w\")\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                img_binary = cv2.cvtColor(img_binary, cv2.COLOR_BGR2GRAY)\n",
    "                # get vertical histogram\n",
    "                ss = img_binary.sum(axis=1)\n",
    "                print(ss[0])\n",
    "                file.write(name)    \n",
    "                for item in ss:\n",
    "                    file.write(\",%.4f\" % item)\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "VHist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The Fourier elliptical features are extracted from each of the images and we proceed to save them in a. txt file.\n",
    "base = \"./\"\n",
    "def CountFlips():\n",
    "    print(\"CountFlips\\n\")\n",
    "    path_EF = base + r\"Feature-Extraction\"\n",
    "    if not os.path.exists(path_EF):\n",
    "        os.makedirs(path_EF)\n",
    "\n",
    "    file = open(base + r\"Feature-Extraction/CountFlips.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "\n",
    "                img_binary = cv2.cvtColor(img_binary, cv2.COLOR_BGR2GRAY)\n",
    "                # count the number of black/white flips on the column of the image \n",
    "                counts = []\n",
    "                for col in range(0,img_binary.shape[1],10):\n",
    "                    first = img_binary[0, col]\n",
    "                    count =0\n",
    "                    for row in range(img_binary.shape[0]):\n",
    "                        if img_binary[row, col] != first:\n",
    "                            count += 1\n",
    "                            first = img_binary[row, col]\n",
    "                        \n",
    "                    counts.append(count)\n",
    "                file.write(name)    \n",
    "                for item in range(len(counts)):\n",
    "                    file.write(\",%.4f\" % counts[item])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "\n",
    "    file.close()\n",
    "CountFlips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NwNGTksVQRs6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EF\n",
      "\n",
      "0_men (1).JPG\n",
      "0_men (10).JPG\n",
      "0_men (100).JPG\n",
      "0_men (101).JPG\n",
      "0_men (102).JPG\n",
      "0_men (103).JPG\n",
      "0_men (104).JPG\n",
      "0_men (105).JPG\n",
      "0_men (106).JPG\n",
      "0_men (107).JPG\n",
      "0_men (108).JPG\n",
      "0_men (109).JPG\n",
      "0_men (11).JPG\n",
      "0_men (110).JPG\n",
      "0_men (111).JPG\n",
      "0_men (112).JPG\n",
      "0_men (113).JPG\n",
      "0_men (114).JPG\n",
      "0_men (115).JPG\n",
      "0_men (116).JPG\n",
      "0_men (117).JPG\n",
      "0_men (118).JPG\n",
      "0_men (119).JPG\n",
      "0_men (12).JPG\n",
      "0_men (120).JPG\n",
      "0_men (121).JPG\n",
      "0_men (122).JPG\n",
      "0_men (123).JPG\n",
      "0_men (124).JPG\n",
      "0_men (125).JPG\n",
      "0_men (126).JPG\n",
      "0_men (127).JPG\n",
      "0_men (128).JPG\n",
      "0_men (129).JPG\n",
      "0_men (13).JPG\n",
      "0_men (130).JPG\n",
      "0_men (131).JPG\n",
      "0_men (132).JPG\n",
      "0_men (133).JPG\n",
      "0_men (134).JPG\n",
      "0_men (135).JPG\n",
      "0_men (136).JPG\n",
      "0_men (137).JPG\n",
      "0_men (138).JPG\n",
      "0_men (139).JPG\n",
      "0_men (14).JPG\n",
      "0_men (140).JPG\n",
      "0_men (141).JPG\n",
      "0_men (142).JPG\n",
      "0_men (143).JPG\n",
      "0_men (144).JPG\n",
      "0_men (145).JPG\n",
      "0_men (146).JPG\n",
      "0_men (147).JPG\n",
      "0_men (148).JPG\n",
      "0_men (149).JPG\n",
      "0_men (15).JPG\n",
      "0_men (150).JPG\n",
      "0_men (151).JPG\n",
      "0_men (152).JPG\n",
      "0_men (153).JPG\n",
      "0_men (154).JPG\n",
      "0_men (155).JPG\n",
      "0_men (156).JPG\n",
      "0_men (157).JPG\n",
      "0_men (158).JPG\n",
      "0_men (159).JPG\n",
      "0_men (16).JPG\n",
      "0_men (160).JPG\n",
      "0_men (161).JPG\n",
      "0_men (162).JPG\n",
      "0_men (163).JPG\n",
      "0_men (164).JPG\n",
      "0_men (165).JPG\n",
      "0_men (166).JPG\n",
      "0_men (167).JPG\n",
      "0_men (168).JPG\n",
      "0_men (169).JPG\n",
      "0_men (17).JPG\n",
      "0_men (170).JPG\n",
      "0_men (171).JPG\n",
      "0_men (172).JPG\n",
      "0_men (18).JPG\n",
      "0_men (19).JPG\n",
      "0_men (2).JPG\n",
      "0_men (20).JPG\n",
      "0_men (21).JPG\n",
      "0_men (22).JPG\n",
      "0_men (23).JPG\n",
      "0_men (24).JPG\n",
      "0_men (25).JPG\n",
      "0_men (26).JPG\n",
      "0_men (27).JPG\n",
      "0_men (28).JPG\n",
      "0_men (29).JPG\n",
      "0_men (3).JPG\n",
      "0_men (30).JPG\n",
      "0_men (31).JPG\n",
      "0_men (32).JPG\n",
      "0_men (33).JPG\n",
      "0_men (34).JPG\n",
      "0_men (35).JPG\n",
      "0_men (36).JPG\n",
      "0_men (37).JPG\n",
      "0_men (38).JPG\n",
      "0_men (39).JPG\n",
      "0_men (4).JPG\n",
      "0_men (40).JPG\n",
      "0_men (41).JPG\n",
      "0_men (42).JPG\n",
      "0_men (43).JPG\n",
      "0_men (44).JPG\n",
      "0_men (45).JPG\n",
      "0_men (46).JPG\n",
      "0_men (47).JPG\n",
      "0_men (48).JPG\n",
      "0_men (49).JPG\n",
      "0_men (5).JPG\n",
      "0_men (50).JPG\n",
      "0_men (51).JPG\n",
      "0_men (52).JPG\n",
      "0_men (53).JPG\n",
      "0_men (54).JPG\n",
      "0_men (55).JPG\n",
      "0_men (56).JPG\n",
      "0_men (57).JPG\n",
      "0_men (58).JPG\n",
      "0_men (59).JPG\n",
      "0_men (6).JPG\n",
      "0_men (60).JPG\n",
      "0_men (61).JPG\n",
      "0_men (62).JPG\n",
      "0_men (63).JPG\n",
      "0_men (64).JPG\n",
      "0_men (65).JPG\n",
      "0_men (66).JPG\n",
      "0_men (67).JPG\n",
      "0_men (68).JPG\n",
      "0_men (69).JPG\n",
      "0_men (7).JPG\n",
      "0_men (70).JPG\n",
      "0_men (71).JPG\n",
      "0_men (72).JPG\n",
      "0_men (73).JPG\n",
      "0_men (74).JPG\n",
      "0_men (75).JPG\n",
      "0_men (76).JPG\n",
      "0_men (77).JPG\n",
      "0_men (78).JPG\n",
      "0_men (79).JPG\n",
      "0_men (8).JPG\n",
      "0_men (80).JPG\n",
      "0_men (81).JPG\n",
      "0_men (82).JPG\n",
      "0_men (83).JPG\n",
      "0_men (84).JPG\n",
      "0_men (85).JPG\n",
      "0_men (86).JPG\n",
      "0_men (87).JPG\n",
      "0_men (88).JPG\n",
      "0_men (89).JPG\n",
      "0_men (9).JPG\n",
      "0_men (90).JPG\n",
      "0_men (91).JPG\n",
      "0_men (92).JPG\n",
      "0_men (93).JPG\n",
      "0_men (94).JPG\n",
      "0_men (95).JPG\n",
      "0_men (96).JPG\n",
      "0_men (97).JPG\n",
      "0_men (98).JPG\n",
      "0_men (99).JPG\n",
      "0_woman (1).JPG\n",
      "0_woman (10).JPG\n",
      "0_woman (100).JPG\n",
      "0_woman (101).JPG\n",
      "0_woman (102).JPG\n",
      "0_woman (103).JPG\n",
      "0_woman (104).JPG\n",
      "0_woman (105).JPG\n",
      "0_woman (106).JPG\n",
      "0_woman (107).JPG\n",
      "0_woman (108).JPG\n",
      "0_woman (109).JPG\n",
      "0_woman (11).JPG\n",
      "0_woman (110).JPG\n",
      "0_woman (111).JPG\n",
      "0_woman (112).JPG\n",
      "0_woman (113).JPG\n",
      "0_woman (114).JPG\n",
      "0_woman (115).JPG\n",
      "0_woman (12).JPG\n",
      "0_woman (13).JPG\n",
      "0_woman (14).JPG\n",
      "0_woman (15).JPG\n",
      "0_woman (16).JPG\n",
      "0_woman (17).JPG\n",
      "0_woman (18).JPG\n",
      "0_woman (19).JPG\n",
      "0_woman (2).JPG\n",
      "0_woman (20).JPG\n",
      "0_woman (21).JPG\n",
      "0_woman (22).JPG\n",
      "0_woman (23).JPG\n",
      "0_woman (24).JPG\n",
      "0_woman (25).JPG\n",
      "0_woman (26).JPG\n",
      "0_woman (27).JPG\n",
      "0_woman (28).JPG\n",
      "0_woman (29).JPG\n",
      "0_woman (3).JPG\n",
      "0_woman (30).JPG\n",
      "0_woman (31).JPG\n",
      "0_woman (32).JPG\n",
      "0_woman (33).JPG\n",
      "0_woman (34).JPG\n",
      "0_woman (35).JPG\n",
      "0_woman (36).JPG\n",
      "0_woman (37).JPG\n",
      "0_woman (38).JPG\n",
      "0_woman (39).JPG\n",
      "0_woman (4).JPG\n",
      "0_woman (40).JPG\n",
      "0_woman (41).JPG\n",
      "0_woman (42).JPG\n",
      "0_woman (43).JPG\n",
      "0_woman (44).JPG\n",
      "0_woman (45).JPG\n",
      "0_woman (46).JPG\n",
      "0_woman (47).JPG\n",
      "0_woman (48).JPG\n",
      "0_woman (49).JPG\n",
      "0_woman (5).JPG\n",
      "0_woman (50).JPG\n",
      "0_woman (51).JPG\n",
      "0_woman (52).JPG\n",
      "0_woman (53).JPG\n",
      "0_woman (54).JPG\n",
      "0_woman (55).JPG\n",
      "0_woman (56).JPG\n",
      "0_woman (57).JPG\n",
      "0_woman (58).JPG\n",
      "0_woman (59).JPG\n",
      "0_woman (6).JPG\n",
      "0_woman (60).JPG\n",
      "0_woman (61).JPG\n",
      "0_woman (62).JPG\n",
      "0_woman (63).JPG\n",
      "0_woman (64).JPG\n",
      "0_woman (65).JPG\n",
      "0_woman (66).JPG\n",
      "0_woman (67).JPG\n",
      "0_woman (68).JPG\n",
      "0_woman (69).JPG\n",
      "0_woman (7).JPG\n",
      "0_woman (70).JPG\n",
      "0_woman (71).JPG\n",
      "0_woman (72).JPG\n",
      "0_woman (73).JPG\n",
      "0_woman (74).JPG\n",
      "0_woman (75).JPG\n",
      "0_woman (76).JPG\n",
      "0_woman (77).JPG\n",
      "0_woman (78).JPG\n",
      "0_woman (79).JPG\n",
      "0_woman (8).JPG\n",
      "0_woman (80).JPG\n",
      "0_woman (81).JPG\n",
      "0_woman (82).JPG\n",
      "0_woman (83).JPG\n",
      "0_woman (84).JPG\n",
      "0_woman (85).JPG\n",
      "0_woman (86).JPG\n",
      "0_woman (87).JPG\n",
      "0_woman (88).JPG\n",
      "0_woman (89).JPG\n",
      "0_woman (9).JPG\n",
      "0_woman (90).JPG\n",
      "0_woman (91).JPG\n",
      "0_woman (92).JPG\n",
      "0_woman (93).JPG\n",
      "0_woman (94).JPG\n",
      "0_woman (95).JPG\n",
      "0_woman (96).JPG\n",
      "0_woman (97).JPG\n",
      "0_woman (98).JPG\n",
      "0_woman (99).JPG\n",
      "1_men (1).JPG\n",
      "1_men (10).JPG\n",
      "1_men (100).JPG\n",
      "1_men (101).JPG\n",
      "1_men (102).JPG\n",
      "1_men (103).JPG\n",
      "1_men (104).JPG\n",
      "1_men (105).JPG\n",
      "1_men (106).JPG\n",
      "1_men (107).JPG\n",
      "1_men (108).JPG\n",
      "1_men (109).JPG\n",
      "1_men (11).JPG\n",
      "1_men (110).JPG\n",
      "1_men (111).JPG\n",
      "1_men (112).JPG\n",
      "1_men (113).JPG\n",
      "1_men (114).JPG\n",
      "1_men (115).JPG\n",
      "1_men (116).JPG\n",
      "1_men (117).JPG\n",
      "1_men (118).JPG\n",
      "1_men (119).JPG\n",
      "1_men (12).JPG\n",
      "1_men (120).JPG\n",
      "1_men (121).JPG\n",
      "1_men (122).JPG\n",
      "1_men (123).JPG\n",
      "1_men (124).JPG\n",
      "1_men (125).JPG\n",
      "1_men (126).JPG\n",
      "1_men (127).JPG\n",
      "1_men (128).JPG\n",
      "1_men (129).JPG\n",
      "1_men (13).JPG\n",
      "1_men (130).JPG\n",
      "1_men (131).JPG\n",
      "1_men (132).JPG\n",
      "1_men (133).JPG\n",
      "1_men (134).JPG\n",
      "1_men (135).JPG\n",
      "1_men (136).JPG\n",
      "1_men (137).JPG\n",
      "1_men (138).JPG\n",
      "1_men (139).JPG\n",
      "1_men (14).JPG\n",
      "1_men (140).JPG\n",
      "1_men (141).JPG\n",
      "1_men (142).JPG\n",
      "1_men (143).JPG\n",
      "1_men (144).JPG\n",
      "1_men (145).JPG\n",
      "1_men (146).JPG\n",
      "1_men (147).JPG\n",
      "1_men (148).JPG\n",
      "1_men (149).JPG\n",
      "1_men (15).JPG\n",
      "1_men (150).JPG\n",
      "1_men (151).JPG\n",
      "1_men (152).JPG\n",
      "1_men (153).JPG\n",
      "1_men (154).JPG\n",
      "1_men (155).JPG\n",
      "1_men (156).JPG\n",
      "1_men (157).JPG\n",
      "1_men (158).JPG\n",
      "1_men (159).JPG\n",
      "1_men (16).JPG\n",
      "1_men (160).JPG\n",
      "1_men (161).JPG\n",
      "1_men (162).JPG\n",
      "1_men (163).JPG\n",
      "1_men (164).JPG\n",
      "1_men (165).JPG\n",
      "1_men (166).JPG\n",
      "1_men (167).JPG\n",
      "1_men (168).JPG\n",
      "1_men (169).JPG\n",
      "1_men (17).JPG\n",
      "1_men (170).JPG\n",
      "1_men (171).JPG\n",
      "1_men (172).JPG\n",
      "1_men (173).JPG\n",
      "1_men (174).JPG\n",
      "1_men (175).JPG\n",
      "1_men (18).JPG\n",
      "1_men (19).JPG\n",
      "1_men (2).JPG\n",
      "1_men (20).JPG\n",
      "1_men (21).JPG\n",
      "1_men (22).JPG\n",
      "1_men (23).JPG\n",
      "1_men (24).JPG\n",
      "1_men (25).JPG\n",
      "1_men (26).JPG\n",
      "1_men (27).JPG\n",
      "1_men (28).JPG\n",
      "1_men (29).JPG\n",
      "1_men (3).JPG\n",
      "1_men (30).JPG\n",
      "1_men (31).JPG\n",
      "1_men (32).JPG\n",
      "1_men (33).JPG\n",
      "1_men (34).JPG\n",
      "1_men (35).JPG\n",
      "1_men (36).JPG\n",
      "1_men (37).JPG\n",
      "1_men (38).JPG\n",
      "1_men (39).JPG\n",
      "1_men (4).JPG\n",
      "1_men (40).JPG\n",
      "1_men (41).JPG\n",
      "1_men (42).JPG\n",
      "1_men (43).JPG\n",
      "1_men (44).JPG\n",
      "1_men (45).JPG\n",
      "1_men (46).JPG\n",
      "1_men (47).JPG\n",
      "1_men (48).JPG\n",
      "1_men (49).JPG\n",
      "1_men (5).JPG\n",
      "1_men (50).JPG\n",
      "1_men (51).JPG\n",
      "1_men (52).JPG\n",
      "1_men (53).JPG\n",
      "1_men (54).JPG\n",
      "1_men (55).JPG\n",
      "1_men (56).JPG\n",
      "1_men (57).JPG\n",
      "1_men (58).JPG\n",
      "1_men (59).JPG\n",
      "1_men (6).JPG\n",
      "1_men (60).JPG\n",
      "1_men (61).JPG\n",
      "1_men (62).JPG\n",
      "1_men (63).JPG\n",
      "1_men (64).JPG\n",
      "1_men (65).JPG\n",
      "1_men (66).JPG\n",
      "1_men (67).JPG\n",
      "1_men (68).JPG\n",
      "1_men (69).JPG\n",
      "1_men (7).JPG\n",
      "1_men (70).JPG\n",
      "1_men (71).JPG\n",
      "1_men (72).JPG\n",
      "1_men (73).JPG\n",
      "1_men (74).JPG\n",
      "1_men (75).JPG\n",
      "1_men (76).JPG\n",
      "1_men (77).JPG\n",
      "1_men (78).JPG\n",
      "1_men (79).JPG\n",
      "1_men (8).JPG\n",
      "1_men (80).JPG\n",
      "1_men (81).JPG\n",
      "1_men (82).JPG\n",
      "1_men (83).JPG\n",
      "1_men (84).JPG\n",
      "1_men (85).JPG\n",
      "1_men (86).JPG\n",
      "1_men (87).JPG\n",
      "1_men (88).JPG\n",
      "1_men (89).JPG\n",
      "1_men (9).JPG\n",
      "1_men (90).JPG\n",
      "1_men (91).JPG\n",
      "1_men (92).JPG\n",
      "1_men (93).JPG\n",
      "1_men (94).JPG\n",
      "1_men (95).JPG\n",
      "1_men (96).JPG\n",
      "1_men (97).JPG\n",
      "1_men (98).JPG\n",
      "1_men (99).JPG\n",
      "1_woman (1).JPG\n",
      "1_woman (10).JPG\n",
      "1_woman (100).JPG\n",
      "1_woman (101).JPG\n",
      "1_woman (102).JPG\n",
      "1_woman (103).JPG\n",
      "1_woman (104).JPG\n",
      "1_woman (105).JPG\n",
      "1_woman (106).JPG\n",
      "1_woman (107).JPG\n",
      "1_woman (108).JPG\n",
      "1_woman (109).JPG\n",
      "1_woman (11).JPG\n",
      "1_woman (110).JPG\n",
      "1_woman (111).JPG\n",
      "1_woman (112).JPG\n",
      "1_woman (113).JPG\n",
      "1_woman (114).JPG\n",
      "1_woman (115).JPG\n",
      "1_woman (116).JPG\n",
      "1_woman (117).JPG\n",
      "1_woman (118).JPG\n",
      "1_woman (119).JPG\n",
      "1_woman (12).JPG\n",
      "1_woman (120).JPG\n",
      "1_woman (121).JPG\n",
      "1_woman (122).JPG\n",
      "1_woman (123).JPG\n",
      "1_woman (124).JPG\n",
      "1_woman (125).JPG\n",
      "1_woman (13).JPG\n",
      "1_woman (14).JPG\n",
      "1_woman (15).JPG\n",
      "1_woman (16).JPG\n",
      "1_woman (17).JPG\n",
      "1_woman (18).JPG\n",
      "1_woman (19).JPG\n",
      "1_woman (2).JPG\n",
      "1_woman (20).JPG\n",
      "1_woman (21).JPG\n",
      "1_woman (22).JPG\n",
      "1_woman (23).JPG\n",
      "1_woman (24).JPG\n",
      "1_woman (25).JPG\n",
      "1_woman (26).JPG\n",
      "1_woman (27).JPG\n",
      "1_woman (28).JPG\n",
      "1_woman (29).JPG\n",
      "1_woman (3).JPG\n",
      "1_woman (30).JPG\n",
      "1_woman (31).JPG\n",
      "1_woman (32).JPG\n",
      "1_woman (33).JPG\n",
      "1_woman (34).JPG\n",
      "1_woman (35).JPG\n",
      "1_woman (36).JPG\n",
      "1_woman (37).JPG\n",
      "1_woman (38).JPG\n",
      "1_woman (39).JPG\n",
      "1_woman (4).JPG\n",
      "1_woman (40).JPG\n",
      "1_woman (41).JPG\n",
      "1_woman (42).JPG\n",
      "1_woman (43).JPG\n",
      "1_woman (44).JPG\n",
      "1_woman (45).JPG\n",
      "1_woman (46).JPG\n",
      "1_woman (47).JPG\n",
      "1_woman (48).JPG\n",
      "1_woman (49).JPG\n",
      "1_woman (5).JPG\n",
      "1_woman (50).JPG\n",
      "1_woman (51).JPG\n",
      "1_woman (52).JPG\n",
      "1_woman (53).JPG\n",
      "1_woman (54).JPG\n",
      "1_woman (55).JPG\n",
      "1_woman (56).JPG\n",
      "1_woman (57).JPG\n",
      "1_woman (58).JPG\n",
      "1_woman (59).JPG\n",
      "1_woman (6).JPG\n",
      "1_woman (60).JPG\n",
      "1_woman (61).JPG\n",
      "1_woman (62).JPG\n",
      "1_woman (63).JPG\n",
      "1_woman (64).JPG\n",
      "1_woman (65).JPG\n",
      "1_woman (66).JPG\n",
      "1_woman (67).JPG\n",
      "1_woman (68).JPG\n",
      "1_woman (69).JPG\n",
      "1_woman (7).JPG\n",
      "1_woman (70).JPG\n",
      "1_woman (71).JPG\n",
      "1_woman (72).JPG\n",
      "1_woman (73).JPG\n",
      "1_woman (74).JPG\n",
      "1_woman (75).JPG\n",
      "1_woman (76).JPG\n",
      "1_woman (77).JPG\n",
      "1_woman (78).JPG\n",
      "1_woman (79).JPG\n",
      "1_woman (8).JPG\n",
      "1_woman (80).JPG\n",
      "1_woman (81).JPG\n",
      "1_woman (82).JPG\n",
      "1_woman (83).JPG\n",
      "1_woman (84).JPG\n",
      "1_woman (85).JPG\n",
      "1_woman (86).JPG\n",
      "1_woman (87).JPG\n",
      "1_woman (88).JPG\n",
      "1_woman (89).JPG\n",
      "1_woman (9).JPG\n",
      "1_woman (90).JPG\n",
      "1_woman (91).JPG\n",
      "1_woman (92).JPG\n",
      "1_woman (93).JPG\n",
      "1_woman (94).JPG\n",
      "1_woman (95).JPG\n",
      "1_woman (96).JPG\n",
      "1_woman (97).JPG\n",
      "1_woman (98).JPG\n",
      "1_woman (99).JPG\n",
      "2_men (1).JPG\n",
      "2_men (100).JPG\n",
      "2_men (101).JPG\n",
      "2_men (102).JPG\n",
      "2_men (103).JPG\n",
      "2_men (104).JPG\n",
      "2_men (105).JPG\n",
      "2_men (106).JPG\n",
      "2_men (109).JPG\n",
      "2_men (11).JPG\n",
      "2_men (110).JPG\n",
      "2_men (111).JPG\n",
      "2_men (112).JPG\n",
      "2_men (113).JPG\n",
      "2_men (114).JPG\n",
      "2_men (115).JPG\n",
      "2_men (116).JPG\n",
      "2_men (117).JPG\n",
      "2_men (118).JPG\n",
      "2_men (119).JPG\n",
      "2_men (12).JPG\n",
      "2_men (120).JPG\n",
      "2_men (121).JPG\n",
      "2_men (122).JPG\n",
      "2_men (123).JPG\n",
      "2_men (124).JPG\n",
      "2_men (125).JPG\n",
      "2_men (126).JPG\n",
      "2_men (127).JPG\n",
      "2_men (128).JPG\n",
      "2_men (129).JPG\n",
      "2_men (13).JPG\n",
      "2_men (130).JPG\n",
      "2_men (131).JPG\n",
      "2_men (132).JPG\n",
      "2_men (133).JPG\n",
      "2_men (134).JPG\n",
      "2_men (135).JPG\n",
      "2_men (136).JPG\n",
      "2_men (137).JPG\n",
      "2_men (138).JPG\n",
      "2_men (139).JPG\n",
      "2_men (14).JPG\n",
      "2_men (140).JPG\n",
      "2_men (141).JPG\n",
      "2_men (142).JPG\n",
      "2_men (143).JPG\n",
      "2_men (144).JPG\n",
      "2_men (145).JPG\n",
      "2_men (146).JPG\n",
      "2_men (147).JPG\n",
      "2_men (148).JPG\n",
      "2_men (149).JPG\n",
      "2_men (15).JPG\n",
      "2_men (150).JPG\n",
      "2_men (151).JPG\n",
      "2_men (152).JPG\n",
      "2_men (153).JPG\n",
      "2_men (154).JPG\n",
      "2_men (155).JPG\n",
      "2_men (156).JPG\n",
      "2_men (157).JPG\n",
      "2_men (158).JPG\n",
      "2_men (159).JPG\n",
      "2_men (16).JPG\n",
      "2_men (160).JPG\n",
      "2_men (161).JPG\n",
      "2_men (162).JPG\n",
      "2_men (163).JPG\n",
      "2_men (164).JPG\n",
      "2_men (165).JPG\n",
      "2_men (166).JPG\n",
      "2_men (167).JPG\n",
      "2_men (168).JPG\n",
      "2_men (169).JPG\n",
      "2_men (17).JPG\n",
      "2_men (170).JPG\n",
      "2_men (171).JPG\n",
      "2_men (172).JPG\n",
      "2_men (173).JPG\n",
      "2_men (174).JPG\n",
      "2_men (175).JPG\n",
      "2_men (176).JPG\n",
      "2_men (177).JPG\n",
      "2_men (18).JPG\n",
      "2_men (19).JPG\n",
      "2_men (2).JPG\n",
      "2_men (20).JPG\n",
      "2_men (21).JPG\n",
      "2_men (22).JPG\n",
      "2_men (23).JPG\n",
      "2_men (24).JPG\n",
      "2_men (25).JPG\n",
      "2_men (26).JPG\n",
      "2_men (27).JPG\n",
      "2_men (28).JPG\n",
      "2_men (29).JPG\n",
      "2_men (3).JPG\n",
      "2_men (30).JPG\n",
      "2_men (31).JPG\n",
      "2_men (32).JPG\n",
      "2_men (33).JPG\n",
      "2_men (34).JPG\n",
      "2_men (35).JPG\n",
      "2_men (36).JPG\n",
      "2_men (37).JPG\n",
      "2_men (38).JPG\n",
      "2_men (39).JPG\n",
      "2_men (4).JPG\n",
      "2_men (40).JPG\n",
      "2_men (41).JPG\n",
      "2_men (42).JPG\n",
      "2_men (43).JPG\n",
      "2_men (44).JPG\n",
      "2_men (45).JPG\n",
      "2_men (46).JPG\n",
      "2_men (47).JPG\n",
      "2_men (48).JPG\n",
      "2_men (49).JPG\n",
      "2_men (5).JPG\n",
      "2_men (50).JPG\n",
      "2_men (51).JPG\n",
      "2_men (52).JPG\n",
      "2_men (53).JPG\n",
      "2_men (54).JPG\n",
      "2_men (55).JPG\n",
      "2_men (56).JPG\n",
      "2_men (57).JPG\n",
      "2_men (58).JPG\n",
      "2_men (59).JPG\n",
      "2_men (6).JPG\n",
      "2_men (60).JPG\n",
      "2_men (61).JPG\n",
      "2_men (62).JPG\n",
      "2_men (63).JPG\n",
      "2_men (64).JPG\n",
      "2_men (65).JPG\n",
      "2_men (66).JPG\n",
      "2_men (67).JPG\n",
      "2_men (68).JPG\n",
      "2_men (69).JPG\n",
      "2_men (70).JPG\n",
      "2_men (71).JPG\n",
      "2_men (72).JPG\n",
      "2_men (73).JPG\n",
      "2_men (74).JPG\n",
      "2_men (75).JPG\n",
      "2_men (76).JPG\n",
      "2_men (77).JPG\n",
      "2_men (78).JPG\n",
      "2_men (79).JPG\n",
      "2_men (8).JPG\n",
      "2_men (80).JPG\n",
      "2_men (81).JPG\n",
      "2_men (82).JPG\n",
      "2_men (83).JPG\n",
      "2_men (84).JPG\n",
      "2_men (85).JPG\n",
      "2_men (86).JPG\n",
      "2_men (87).JPG\n",
      "2_men (88).JPG\n",
      "2_men (89).JPG\n",
      "2_men (9).JPG\n",
      "2_men (90).JPG\n",
      "2_men (91).JPG\n",
      "2_men (92).JPG\n",
      "2_men (93).JPG\n",
      "2_men (94).JPG\n",
      "2_men (95).JPG\n",
      "2_men (96).JPG\n",
      "2_men (97).JPG\n",
      "2_men (98).JPG\n",
      "2_men (99).JPG\n",
      "2_woman (1).JPG\n",
      "2_woman (10).JPG\n",
      "2_woman (100).JPG\n",
      "2_woman (101).JPG\n",
      "2_woman (102).JPG\n",
      "2_woman (103).JPG\n",
      "2_woman (104).JPG\n",
      "2_woman (105).JPG\n",
      "2_woman (106).JPG\n",
      "2_woman (107).JPG\n",
      "2_woman (108).JPG\n",
      "2_woman (109).JPG\n",
      "2_woman (11).JPG\n",
      "2_woman (110).JPG\n",
      "2_woman (111).JPG\n",
      "2_woman (112).JPG\n",
      "2_woman (113).JPG\n",
      "2_woman (114).JPG\n",
      "2_woman (115).JPG\n",
      "2_woman (116).JPG\n",
      "2_woman (117).JPG\n",
      "2_woman (118).JPG\n",
      "2_woman (119).JPG\n",
      "2_woman (12).JPG\n",
      "2_woman (120).JPG\n",
      "2_woman (121).JPG\n",
      "2_woman (122).JPG\n",
      "2_woman (123).JPG\n",
      "2_woman (124).JPG\n",
      "2_woman (125).JPG\n",
      "2_woman (126).JPG\n",
      "2_woman (127).JPG\n",
      "2_woman (128).JPG\n",
      "2_woman (129).JPG\n",
      "2_woman (13).JPG\n",
      "2_woman (130).JPG\n",
      "2_woman (131).JPG\n",
      "2_woman (132).JPG\n",
      "2_woman (133).JPG\n",
      "2_woman (14).JPG\n",
      "2_woman (15).JPG\n",
      "2_woman (16).JPG\n",
      "2_woman (17).JPG\n",
      "2_woman (18).JPG\n",
      "2_woman (19).JPG\n",
      "2_woman (2).JPG\n",
      "2_woman (20).JPG\n",
      "2_woman (21).JPG\n",
      "2_woman (22).JPG\n",
      "2_woman (23).JPG\n",
      "2_woman (24).JPG\n",
      "2_woman (25).JPG\n",
      "2_woman (26).JPG\n",
      "2_woman (27).JPG\n",
      "2_woman (28).JPG\n",
      "2_woman (29).JPG\n",
      "2_woman (3).JPG\n",
      "2_woman (30).JPG\n",
      "2_woman (31).JPG\n",
      "2_woman (32).JPG\n",
      "2_woman (33).JPG\n",
      "2_woman (34).JPG\n",
      "2_woman (35).JPG\n",
      "2_woman (36).JPG\n",
      "2_woman (37).JPG\n",
      "2_woman (38).JPG\n",
      "2_woman (39).JPG\n",
      "2_woman (4).JPG\n",
      "2_woman (40).JPG\n",
      "2_woman (41).JPG\n",
      "2_woman (42).JPG\n",
      "2_woman (43).JPG\n",
      "2_woman (44).JPG\n",
      "2_woman (45).JPG\n",
      "2_woman (46).JPG\n",
      "2_woman (47).JPG\n",
      "2_woman (48).JPG\n",
      "2_woman (49).JPG\n",
      "2_woman (5).JPG\n",
      "2_woman (50).JPG\n",
      "2_woman (51).JPG\n",
      "2_woman (52).JPG\n",
      "2_woman (53).JPG\n",
      "2_woman (54).JPG\n",
      "2_woman (55).JPG\n",
      "2_woman (56).JPG\n",
      "2_woman (57).JPG\n",
      "2_woman (58).JPG\n",
      "2_woman (59).JPG\n",
      "2_woman (6).JPG\n",
      "2_woman (60).JPG\n",
      "2_woman (61).JPG\n",
      "2_woman (62).JPG\n",
      "2_woman (63).JPG\n",
      "2_woman (64).JPG\n",
      "2_woman (65).JPG\n",
      "2_woman (66).JPG\n",
      "2_woman (67).JPG\n",
      "2_woman (68).JPG\n",
      "2_woman (69).JPG\n",
      "2_woman (7).JPG\n",
      "2_woman (70).JPG\n",
      "2_woman (71).JPG\n",
      "2_woman (72).JPG\n",
      "2_woman (73).JPG\n",
      "2_woman (74).JPG\n",
      "2_woman (75).JPG\n",
      "2_woman (76).JPG\n",
      "2_woman (77).JPG\n",
      "2_woman (78).JPG\n",
      "2_woman (79).JPG\n",
      "2_woman (8).JPG\n",
      "2_woman (80).JPG\n",
      "2_woman (81).JPG\n",
      "2_woman (82).JPG\n",
      "2_woman (83).JPG\n",
      "2_woman (84).JPG\n",
      "2_woman (85).JPG\n",
      "2_woman (86).JPG\n",
      "2_woman (87).JPG\n",
      "2_woman (88).JPG\n",
      "2_woman (89).JPG\n",
      "2_woman (9).JPG\n",
      "2_woman (90).JPG\n",
      "2_woman (91).JPG\n",
      "2_woman (92).JPG\n",
      "2_woman (93).JPG\n",
      "2_woman (94).JPG\n",
      "2_woman (95).JPG\n",
      "2_woman (96).JPG\n",
      "2_woman (97).JPG\n",
      "2_woman (98).JPG\n",
      "2_woman (99).JPG\n",
      "3_men (1).JPG\n",
      "3_men (10).JPG\n",
      "3_men (100).JPG\n",
      "3_men (101).JPG\n",
      "3_men (102).JPG\n",
      "3_men (103).JPG\n",
      "3_men (104).JPG\n",
      "3_men (105).JPG\n",
      "3_men (106).JPG\n",
      "3_men (107).JPG\n",
      "3_men (108).JPG\n",
      "3_men (109).JPG\n",
      "3_men (110).JPG\n",
      "3_men (111).JPG\n",
      "3_men (112).JPG\n",
      "3_men (113).JPG\n",
      "3_men (114).JPG\n",
      "3_men (115).JPG\n",
      "3_men (116).JPG\n",
      "3_men (117).JPG\n",
      "3_men (118).JPG\n",
      "3_men (119).JPG\n",
      "3_men (120).JPG\n",
      "3_men (121).JPG\n",
      "3_men (122).JPG\n",
      "3_men (123).JPG\n",
      "3_men (124).JPG\n",
      "3_men (125).JPG\n",
      "3_men (126).JPG\n",
      "3_men (127).JPG\n",
      "3_men (128).JPG\n",
      "3_men (129).JPG\n",
      "3_men (130).JPG\n",
      "3_men (131).JPG\n",
      "3_men (132).JPG\n",
      "3_men (133).JPG\n",
      "3_men (135).JPG\n",
      "3_men (136).JPG\n",
      "3_men (137).JPG\n",
      "3_men (138).JPG\n",
      "3_men (139).JPG\n",
      "3_men (14).JPG\n",
      "3_men (142).JPG\n",
      "3_men (143).JPG\n",
      "3_men (144).JPG\n",
      "3_men (145).JPG\n",
      "3_men (146).JPG\n",
      "3_men (147).JPG\n",
      "3_men (148).JPG\n",
      "3_men (149).JPG\n",
      "3_men (15).JPG\n",
      "3_men (150).JPG\n",
      "3_men (151).JPG\n",
      "3_men (152).JPG\n",
      "3_men (153).JPG\n",
      "3_men (154).JPG\n",
      "3_men (155).JPG\n",
      "3_men (156).JPG\n",
      "3_men (157).JPG\n",
      "3_men (158).JPG\n",
      "3_men (159).JPG\n",
      "3_men (16).JPG\n",
      "3_men (160).JPG\n",
      "3_men (161).JPG\n",
      "3_men (162).JPG\n",
      "3_men (163).JPG\n",
      "3_men (164).JPG\n",
      "3_men (165).JPG\n",
      "3_men (166).JPG\n",
      "3_men (167).JPG\n",
      "3_men (168).JPG\n",
      "3_men (169).JPG\n",
      "3_men (17).JPG\n",
      "3_men (170).JPG\n",
      "3_men (171).JPG\n",
      "3_men (172).JPG\n",
      "3_men (173).JPG\n",
      "3_men (174).JPG\n",
      "3_men (175).JPG\n",
      "3_men (176).JPG\n",
      "3_men (177).JPG\n",
      "3_men (178).JPG\n",
      "3_men (179).JPG\n",
      "3_men (18).JPG\n",
      "3_men (180).JPG\n",
      "3_men (181).JPG\n",
      "3_men (182).JPG\n",
      "3_men (19).JPG\n",
      "3_men (2).JPG\n",
      "3_men (20).JPG\n",
      "3_men (21).JPG\n",
      "3_men (22).JPG\n",
      "3_men (23).JPG\n",
      "3_men (24).JPG\n",
      "3_men (25).JPG\n",
      "3_men (26).JPG\n",
      "3_men (27).JPG\n",
      "3_men (28).JPG\n",
      "3_men (29).JPG\n",
      "3_men (3).JPG\n",
      "3_men (30).JPG\n",
      "3_men (31).JPG\n",
      "3_men (32).JPG\n",
      "3_men (33).JPG\n",
      "3_men (35).JPG\n",
      "3_men (36).JPG\n",
      "3_men (37).JPG\n",
      "3_men (39).JPG\n",
      "3_men (4).JPG\n",
      "3_men (40).JPG\n",
      "3_men (41).JPG\n",
      "3_men (42).JPG\n",
      "3_men (43).JPG\n",
      "3_men (44).JPG\n",
      "3_men (45).JPG\n",
      "3_men (46).JPG\n",
      "3_men (48).JPG\n",
      "3_men (49).JPG\n",
      "3_men (5).JPG\n",
      "3_men (50).JPG\n",
      "3_men (51).JPG\n",
      "3_men (52).JPG\n",
      "3_men (53).JPG\n",
      "3_men (54).JPG\n",
      "3_men (55).JPG\n",
      "3_men (56).JPG\n",
      "3_men (57).JPG\n",
      "3_men (58).JPG\n",
      "3_men (59).JPG\n",
      "3_men (6).JPG\n",
      "3_men (60).JPG\n",
      "3_men (61).JPG\n",
      "3_men (62).JPG\n",
      "3_men (63).JPG\n",
      "3_men (64).JPG\n",
      "3_men (65).JPG\n",
      "3_men (66).JPG\n",
      "3_men (67).JPG\n",
      "3_men (68).JPG\n",
      "3_men (69).JPG\n",
      "3_men (7).JPG\n",
      "3_men (70).JPG\n",
      "3_men (71).JPG\n",
      "3_men (72).JPG\n",
      "3_men (73).JPG\n",
      "3_men (74).JPG\n",
      "3_men (75).JPG\n",
      "3_men (76).JPG\n",
      "3_men (77).JPG\n",
      "3_men (78).JPG\n",
      "3_men (79).JPG\n",
      "3_men (8).JPG\n",
      "3_men (80).JPG\n",
      "3_men (81).JPG\n",
      "3_men (82).JPG\n",
      "3_men (83).JPG\n",
      "3_men (84).JPG\n",
      "3_men (85).JPG\n",
      "3_men (86).JPG\n",
      "3_men (87).JPG\n",
      "3_men (89).JPG\n",
      "3_men (9).JPG\n",
      "3_men (90).JPG\n",
      "3_men (91).JPG\n",
      "3_men (92).JPG\n",
      "3_men (93).JPG\n",
      "3_men (94).JPG\n",
      "3_men (95).JPG\n",
      "3_men (96).JPG\n",
      "3_men (97).JPG\n",
      "3_men (98).JPG\n",
      "3_men (99).JPG\n",
      "3_woman (1).JPG\n",
      "3_woman (10).JPG\n",
      "3_woman (100).JPG\n",
      "3_woman (101).JPG\n",
      "3_woman (102).JPG\n",
      "3_woman (103).JPG\n",
      "3_woman (104).JPG\n",
      "3_woman (105).JPG\n",
      "3_woman (106).JPG\n",
      "3_woman (107).JPG\n",
      "3_woman (108).JPG\n",
      "3_woman (109).JPG\n",
      "3_woman (11).JPG\n",
      "3_woman (110).JPG\n",
      "3_woman (111).JPG\n",
      "3_woman (112).JPG\n",
      "3_woman (113).JPG\n",
      "3_woman (114).JPG\n",
      "3_woman (115).JPG\n",
      "3_woman (116).JPG\n",
      "3_woman (117).JPG\n",
      "3_woman (118).JPG\n",
      "3_woman (119).JPG\n",
      "3_woman (12).JPG\n",
      "3_woman (120).JPG\n",
      "3_woman (121).JPG\n",
      "3_woman (122).JPG\n",
      "3_woman (123).JPG\n",
      "3_woman (124).JPG\n",
      "3_woman (125).JPG\n",
      "3_woman (126).JPG\n",
      "3_woman (127).JPG\n",
      "3_woman (128).JPG\n",
      "3_woman (129).JPG\n",
      "3_woman (13).JPG\n",
      "3_woman (130).JPG\n",
      "3_woman (131).JPG\n",
      "3_woman (132).JPG\n",
      "3_woman (133).JPG\n",
      "3_woman (134).JPG\n",
      "3_woman (135).JPG\n",
      "3_woman (136).JPG\n",
      "3_woman (14).JPG\n",
      "3_woman (15).JPG\n",
      "3_woman (16).JPG\n",
      "3_woman (17).JPG\n",
      "3_woman (18).JPG\n",
      "3_woman (19).JPG\n",
      "3_woman (2).JPG\n",
      "3_woman (20).JPG\n",
      "3_woman (21).JPG\n",
      "3_woman (22).JPG\n",
      "3_woman (23).JPG\n",
      "3_woman (24).JPG\n",
      "3_woman (25).JPG\n",
      "3_woman (26).JPG\n",
      "3_woman (27).JPG\n",
      "3_woman (28).JPG\n",
      "3_woman (29).JPG\n",
      "3_woman (3).JPG\n",
      "3_woman (30).JPG\n",
      "3_woman (31).JPG\n",
      "3_woman (32).JPG\n",
      "3_woman (33).JPG\n",
      "3_woman (34).JPG\n",
      "3_woman (35).JPG\n",
      "3_woman (36).JPG\n",
      "3_woman (37).JPG\n",
      "3_woman (38).JPG\n",
      "3_woman (39).JPG\n",
      "3_woman (4).JPG\n",
      "3_woman (40).JPG\n",
      "3_woman (41).JPG\n",
      "3_woman (42).JPG\n",
      "3_woman (43).JPG\n",
      "3_woman (44).JPG\n",
      "3_woman (45).JPG\n",
      "3_woman (46).JPG\n",
      "3_woman (47).JPG\n",
      "3_woman (48).JPG\n",
      "3_woman (49).JPG\n",
      "3_woman (5).JPG\n",
      "3_woman (50).JPG\n",
      "3_woman (51).JPG\n",
      "3_woman (52).JPG\n",
      "3_woman (53).JPG\n",
      "3_woman (54).JPG\n",
      "3_woman (55).JPG\n",
      "3_woman (56).JPG\n",
      "3_woman (57).JPG\n",
      "3_woman (58).JPG\n",
      "3_woman (59).JPG\n",
      "3_woman (6).JPG\n",
      "3_woman (60).JPG\n",
      "3_woman (61).JPG\n",
      "3_woman (62).JPG\n",
      "3_woman (63).JPG\n",
      "3_woman (64).JPG\n",
      "3_woman (65).JPG\n",
      "3_woman (66).JPG\n",
      "3_woman (67).JPG\n",
      "3_woman (68).JPG\n",
      "3_woman (69).JPG\n",
      "3_woman (7).JPG\n",
      "3_woman (70).JPG\n",
      "3_woman (71).JPG\n",
      "3_woman (72).JPG\n",
      "3_woman (73).JPG\n",
      "3_woman (74).JPG\n",
      "3_woman (75).JPG\n",
      "3_woman (76).JPG\n",
      "3_woman (77).JPG\n",
      "3_woman (78).JPG\n",
      "3_woman (79).JPG\n",
      "3_woman (8).JPG\n",
      "3_woman (80).JPG\n",
      "3_woman (81).JPG\n",
      "3_woman (82).JPG\n",
      "3_woman (83).JPG\n",
      "3_woman (84).JPG\n",
      "3_woman (85).JPG\n",
      "3_woman (86).JPG\n",
      "3_woman (87).JPG\n",
      "3_woman (88).JPG\n",
      "3_woman (89).JPG\n",
      "3_woman (9).JPG\n",
      "3_woman (90).JPG\n",
      "3_woman (91).JPG\n",
      "3_woman (92).JPG\n",
      "3_woman (93).JPG\n",
      "3_woman (94).JPG\n",
      "3_woman (95).JPG\n",
      "3_woman (96).JPG\n",
      "3_woman (97).JPG\n",
      "3_woman (98).JPG\n",
      "3_woman (99).JPG\n",
      "4_men (1).JPG\n",
      "4_men (10).JPG\n",
      "4_men (100).JPG\n",
      "4_men (101).JPG\n",
      "4_men (102).JPG\n",
      "4_men (103).JPG\n",
      "4_men (104).JPG\n",
      "4_men (105).JPG\n",
      "4_men (106).JPG\n",
      "4_men (107).JPG\n",
      "4_men (108).JPG\n",
      "4_men (109).JPG\n",
      "4_men (11).JPG\n",
      "4_men (110).JPG\n",
      "4_men (111).JPG\n",
      "4_men (112).JPG\n",
      "4_men (113).JPG\n",
      "4_men (114).JPG\n",
      "4_men (115).JPG\n",
      "4_men (116).JPG\n",
      "4_men (117).JPG\n",
      "4_men (118).JPG\n",
      "4_men (119).JPG\n",
      "4_men (12).JPG\n",
      "4_men (120).JPG\n",
      "4_men (121).JPG\n",
      "4_men (122).JPG\n",
      "4_men (123).JPG\n",
      "4_men (124).JPG\n",
      "4_men (125).JPG\n",
      "4_men (126).JPG\n",
      "4_men (127).JPG\n",
      "4_men (128).JPG\n",
      "4_men (129).JPG\n",
      "4_men (13).JPG\n",
      "4_men (130).JPG\n",
      "4_men (131).JPG\n",
      "4_men (132).JPG\n",
      "4_men (133).JPG\n",
      "4_men (134).JPG\n",
      "4_men (135).JPG\n",
      "4_men (136).JPG\n",
      "4_men (137).JPG\n",
      "4_men (138).JPG\n",
      "4_men (139).JPG\n",
      "4_men (14).JPG\n",
      "4_men (140).JPG\n",
      "4_men (142).JPG\n",
      "4_men (143).JPG\n",
      "4_men (144).JPG\n",
      "4_men (145).JPG\n",
      "4_men (146).JPG\n",
      "4_men (147).JPG\n",
      "4_men (15).JPG\n",
      "4_men (151).JPG\n",
      "4_men (152).JPG\n",
      "4_men (153).JPG\n",
      "4_men (154).JPG\n",
      "4_men (155).JPG\n",
      "4_men (156).JPG\n",
      "4_men (157).JPG\n",
      "4_men (158).JPG\n",
      "4_men (159).JPG\n",
      "4_men (16).JPG\n",
      "4_men (160).JPG\n",
      "4_men (161).JPG\n",
      "4_men (162).JPG\n",
      "4_men (163).JPG\n",
      "4_men (164).JPG\n",
      "4_men (165).JPG\n",
      "4_men (166).JPG\n",
      "4_men (167).JPG\n",
      "4_men (168).JPG\n",
      "4_men (169).JPG\n",
      "4_men (17).JPG\n",
      "4_men (170).JPG\n",
      "4_men (171).JPG\n",
      "4_men (172).JPG\n",
      "4_men (173).JPG\n",
      "4_men (174).JPG\n",
      "4_men (18).JPG\n",
      "4_men (19).JPG\n",
      "4_men (2).JPG\n",
      "4_men (20).JPG\n",
      "4_men (21).JPG\n",
      "4_men (22).JPG\n",
      "4_men (23).JPG\n",
      "4_men (24).JPG\n",
      "4_men (25).JPG\n",
      "4_men (27).JPG\n",
      "4_men (28).JPG\n",
      "4_men (29).JPG\n",
      "4_men (3).JPG\n",
      "4_men (30).JPG\n",
      "4_men (31).JPG\n",
      "4_men (32).JPG\n",
      "4_men (33).JPG\n",
      "4_men (34).JPG\n",
      "4_men (35).JPG\n",
      "4_men (36).JPG\n",
      "4_men (37).JPG\n",
      "4_men (38).JPG\n",
      "4_men (39).JPG\n",
      "4_men (4).JPG\n",
      "4_men (40).JPG\n",
      "4_men (41).JPG\n",
      "4_men (42).JPG\n",
      "4_men (43).JPG\n",
      "4_men (44).JPG\n",
      "4_men (45).JPG\n",
      "4_men (46).JPG\n",
      "4_men (47).JPG\n",
      "4_men (48).JPG\n",
      "4_men (49).JPG\n",
      "4_men (50).JPG\n",
      "4_men (51).JPG\n",
      "4_men (52).JPG\n",
      "4_men (53).JPG\n",
      "4_men (54).JPG\n",
      "4_men (56).JPG\n",
      "4_men (57).JPG\n",
      "4_men (58).JPG\n",
      "4_men (59).JPG\n",
      "4_men (60).JPG\n",
      "4_men (61).JPG\n",
      "4_men (62).JPG\n",
      "4_men (64).JPG\n",
      "4_men (65).JPG\n",
      "4_men (66).JPG\n",
      "4_men (67).JPG\n",
      "4_men (68).JPG\n",
      "4_men (69).JPG\n",
      "4_men (7).JPG\n",
      "4_men (70).JPG\n",
      "4_men (71).JPG\n",
      "4_men (72).JPG\n",
      "4_men (73).JPG\n",
      "4_men (74).JPG\n",
      "4_men (75).JPG\n",
      "4_men (76).JPG\n",
      "4_men (77).JPG\n",
      "4_men (78).JPG\n",
      "4_men (79).JPG\n",
      "4_men (8).JPG\n",
      "4_men (80).JPG\n",
      "4_men (81).JPG\n",
      "4_men (82).JPG\n",
      "4_men (83).JPG\n",
      "4_men (84).JPG\n",
      "4_men (85).JPG\n",
      "4_men (86).JPG\n",
      "4_men (87).JPG\n",
      "4_men (88).JPG\n",
      "4_men (89).JPG\n",
      "4_men (9).JPG\n",
      "4_men (90).JPG\n",
      "4_men (91).JPG\n",
      "4_men (92).JPG\n",
      "4_men (93).JPG\n",
      "4_men (94).JPG\n",
      "4_men (95).JPG\n",
      "4_men (96).JPG\n",
      "4_men (97).JPG\n",
      "4_men (98).JPG\n",
      "4_men (99).JPG\n",
      "4_woman (1).JPG\n",
      "4_woman (10).JPG\n",
      "4_woman (100).JPG\n",
      "4_woman (101).JPG\n",
      "4_woman (102).JPG\n",
      "4_woman (103).JPG\n",
      "4_woman (104).JPG\n",
      "4_woman (105).JPG\n",
      "4_woman (106).JPG\n",
      "4_woman (107).JPG\n",
      "4_woman (108).JPG\n",
      "4_woman (109).JPG\n",
      "4_woman (11).JPG\n",
      "4_woman (110).JPG\n",
      "4_woman (111).JPG\n",
      "4_woman (112).JPG\n",
      "4_woman (113).JPG\n",
      "4_woman (114).JPG\n",
      "4_woman (115).JPG\n",
      "4_woman (116).JPG\n",
      "4_woman (117).JPG\n",
      "4_woman (118).JPG\n",
      "4_woman (119).JPG\n",
      "4_woman (12).JPG\n",
      "4_woman (120).JPG\n",
      "4_woman (121).JPG\n",
      "4_woman (122).JPG\n",
      "4_woman (123).JPG\n",
      "4_woman (124).JPG\n",
      "4_woman (125).JPG\n",
      "4_woman (126).JPG\n",
      "4_woman (127).JPG\n",
      "4_woman (128).JPG\n",
      "4_woman (129).JPG\n",
      "4_woman (13).JPG\n",
      "4_woman (130).JPG\n",
      "4_woman (131).JPG\n",
      "4_woman (132).JPG\n",
      "4_woman (133).JPG\n",
      "4_woman (14).JPG\n",
      "4_woman (15).JPG\n",
      "4_woman (16).JPG\n",
      "4_woman (17).JPG\n",
      "4_woman (18).JPG\n",
      "4_woman (19).JPG\n",
      "4_woman (2).JPG\n",
      "4_woman (20).JPG\n",
      "4_woman (21).JPG\n",
      "4_woman (22).JPG\n",
      "4_woman (23).JPG\n",
      "4_woman (24).JPG\n",
      "4_woman (25).JPG\n",
      "4_woman (26).JPG\n",
      "4_woman (27).JPG\n",
      "4_woman (28).JPG\n",
      "4_woman (29).JPG\n",
      "4_woman (3).JPG\n",
      "4_woman (30).JPG\n",
      "4_woman (31).JPG\n",
      "4_woman (32).JPG\n",
      "4_woman (33).JPG\n",
      "4_woman (34).JPG\n",
      "4_woman (35).JPG\n",
      "4_woman (36).JPG\n",
      "4_woman (37).JPG\n",
      "4_woman (38).JPG\n",
      "4_woman (39).JPG\n",
      "4_woman (4).JPG\n",
      "4_woman (40).JPG\n",
      "4_woman (41).JPG\n",
      "4_woman (42).JPG\n",
      "4_woman (43).JPG\n",
      "4_woman (44).JPG\n",
      "4_woman (45).JPG\n",
      "4_woman (46).JPG\n",
      "4_woman (47).JPG\n",
      "4_woman (48).JPG\n",
      "4_woman (49).JPG\n",
      "4_woman (5).JPG\n",
      "4_woman (50).JPG\n",
      "4_woman (51).JPG\n",
      "4_woman (52).JPG\n",
      "4_woman (53).JPG\n",
      "4_woman (54).JPG\n",
      "4_woman (55).JPG\n",
      "4_woman (56).JPG\n",
      "4_woman (57).JPG\n",
      "4_woman (58).JPG\n",
      "4_woman (59).JPG\n",
      "4_woman (6).JPG\n",
      "4_woman (60).JPG\n",
      "4_woman (61).JPG\n",
      "4_woman (62).JPG\n",
      "4_woman (63).JPG\n",
      "4_woman (64).JPG\n",
      "4_woman (65).JPG\n",
      "4_woman (66).JPG\n",
      "4_woman (67).JPG\n",
      "4_woman (68).JPG\n",
      "4_woman (69).JPG\n",
      "4_woman (7).JPG\n",
      "4_woman (70).JPG\n",
      "4_woman (71).JPG\n",
      "4_woman (72).JPG\n",
      "4_woman (73).JPG\n",
      "4_woman (74).JPG\n",
      "4_woman (75).JPG\n",
      "4_woman (76).JPG\n",
      "4_woman (77).JPG\n",
      "4_woman (78).JPG\n",
      "4_woman (8).JPG\n",
      "4_woman (80).JPG\n",
      "4_woman (81).JPG\n",
      "4_woman (82).JPG\n",
      "4_woman (83).JPG\n",
      "4_woman (84).JPG\n",
      "4_woman (85).JPG\n",
      "4_woman (86).JPG\n",
      "4_woman (87).JPG\n",
      "4_woman (88).JPG\n",
      "4_woman (89).JPG\n",
      "4_woman (9).JPG\n",
      "4_woman (90).JPG\n",
      "4_woman (91).JPG\n",
      "4_woman (92).JPG\n",
      "4_woman (93).JPG\n",
      "4_woman (94).JPG\n",
      "4_woman (95).JPG\n",
      "4_woman (96).JPG\n",
      "4_woman (97).JPG\n",
      "4_woman (98).JPG\n",
      "4_woman (99).JPG\n",
      "5_men (1).JPG\n",
      "5_men (10).JPG\n",
      "5_men (100).JPG\n",
      "5_men (101).JPG\n",
      "5_men (102).JPG\n",
      "5_men (103).JPG\n",
      "5_men (104).JPG\n",
      "5_men (105).JPG\n",
      "5_men (106).JPG\n",
      "5_men (107).JPG\n",
      "5_men (108).JPG\n",
      "5_men (109).JPG\n",
      "5_men (11).JPG\n",
      "5_men (110).JPG\n",
      "5_men (111).JPG\n",
      "5_men (112).JPG\n",
      "5_men (113).JPG\n",
      "5_men (114).JPG\n",
      "5_men (115).JPG\n",
      "5_men (116).JPG\n",
      "5_men (117).JPG\n",
      "5_men (118).JPG\n",
      "5_men (119).JPG\n",
      "5_men (12).JPG\n",
      "5_men (120).JPG\n",
      "5_men (121).JPG\n",
      "5_men (122).JPG\n",
      "5_men (123).JPG\n",
      "5_men (124).JPG\n",
      "5_men (125).JPG\n",
      "5_men (126).JPG\n",
      "5_men (127).JPG\n",
      "5_men (128).JPG\n",
      "5_men (129).JPG\n",
      "5_men (13).JPG\n",
      "5_men (130).JPG\n",
      "5_men (131).JPG\n",
      "5_men (132).JPG\n",
      "5_men (133).JPG\n",
      "5_men (134).JPG\n",
      "5_men (135).JPG\n",
      "5_men (136).JPG\n",
      "5_men (137).JPG\n",
      "5_men (138).JPG\n",
      "5_men (139).JPG\n",
      "5_men (14).JPG\n",
      "5_men (140).JPG\n",
      "5_men (141).JPG\n",
      "5_men (142).JPG\n",
      "5_men (143).JPG\n",
      "5_men (144).JPG\n",
      "5_men (145).JPG\n",
      "5_men (146).JPG\n",
      "5_men (147).JPG\n",
      "5_men (148).JPG\n",
      "5_men (149).JPG\n",
      "5_men (15).JPG\n",
      "5_men (150).JPG\n",
      "5_men (151).JPG\n",
      "5_men (152).JPG\n",
      "5_men (153).JPG\n",
      "5_men (154).JPG\n",
      "5_men (155).JPG\n",
      "5_men (156).JPG\n",
      "5_men (157).JPG\n",
      "5_men (158).JPG\n",
      "5_men (159).JPG\n",
      "5_men (16).JPG\n",
      "5_men (160).JPG\n",
      "5_men (161).JPG\n",
      "5_men (162).JPG\n",
      "5_men (163).JPG\n",
      "5_men (164).JPG\n",
      "5_men (165).JPG\n",
      "5_men (166).JPG\n",
      "5_men (167).JPG\n",
      "5_men (168).JPG\n",
      "5_men (169).JPG\n",
      "5_men (17).JPG\n",
      "5_men (170).JPG\n",
      "5_men (171).JPG\n",
      "5_men (172).JPG\n",
      "5_men (173).JPG\n",
      "5_men (174).JPG\n",
      "5_men (175).JPG\n",
      "5_men (176).JPG\n",
      "5_men (18).JPG\n",
      "5_men (19).JPG\n",
      "5_men (2).JPG\n",
      "5_men (20).JPG\n",
      "5_men (21).JPG\n",
      "5_men (22).JPG\n",
      "5_men (23).JPG\n",
      "5_men (24).JPG\n",
      "5_men (25).JPG\n",
      "5_men (26).JPG\n",
      "5_men (27).JPG\n",
      "5_men (28).JPG\n",
      "5_men (29).JPG\n",
      "5_men (3).JPG\n",
      "5_men (30).JPG\n",
      "5_men (31).JPG\n",
      "5_men (32).JPG\n",
      "5_men (33).JPG\n",
      "5_men (34).JPG\n",
      "5_men (35).JPG\n",
      "5_men (36).JPG\n",
      "5_men (37).JPG\n",
      "5_men (38).JPG\n",
      "5_men (39).JPG\n",
      "5_men (4).JPG\n",
      "5_men (40).JPG\n",
      "5_men (41).JPG\n",
      "5_men (42).JPG\n",
      "5_men (43).JPG\n",
      "5_men (44).JPG\n",
      "5_men (45).JPG\n",
      "5_men (46).JPG\n",
      "5_men (47).JPG\n",
      "5_men (48).JPG\n",
      "5_men (49).JPG\n",
      "5_men (5).JPG\n",
      "5_men (50).JPG\n",
      "5_men (51).JPG\n",
      "5_men (52).JPG\n",
      "5_men (53).JPG\n",
      "5_men (54).JPG\n",
      "5_men (55).JPG\n",
      "5_men (56).JPG\n",
      "5_men (57).JPG\n",
      "5_men (58).JPG\n",
      "5_men (59).JPG\n",
      "5_men (6).JPG\n",
      "5_men (60).JPG\n",
      "5_men (61).JPG\n",
      "5_men (62).JPG\n",
      "5_men (63).JPG\n",
      "5_men (64).JPG\n",
      "5_men (65).JPG\n",
      "5_men (66).JPG\n",
      "5_men (67).JPG\n",
      "5_men (68).JPG\n",
      "5_men (69).JPG\n",
      "5_men (7).JPG\n",
      "5_men (70).JPG\n",
      "5_men (71).JPG\n",
      "5_men (72).JPG\n",
      "5_men (73).JPG\n",
      "5_men (74).JPG\n",
      "5_men (75).JPG\n",
      "5_men (76).JPG\n",
      "5_men (77).JPG\n",
      "5_men (78).JPG\n",
      "5_men (79).JPG\n",
      "5_men (8).JPG\n",
      "5_men (80).JPG\n",
      "5_men (81).JPG\n",
      "5_men (82).JPG\n",
      "5_men (83).JPG\n",
      "5_men (84).JPG\n",
      "5_men (85).JPG\n",
      "5_men (86).JPG\n",
      "5_men (87).JPG\n",
      "5_men (88).JPG\n",
      "5_men (89).JPG\n",
      "5_men (9).JPG\n",
      "5_men (90).JPG\n",
      "5_men (91).JPG\n",
      "5_men (92).JPG\n",
      "5_men (93).JPG\n",
      "5_men (94).JPG\n",
      "5_men (95).JPG\n",
      "5_men (96).JPG\n",
      "5_men (97).JPG\n",
      "5_men (98).JPG\n",
      "5_men (99).JPG\n",
      "5_men.JPG\n",
      "5_woman (1).JPG\n",
      "5_woman (10).JPG\n",
      "5_woman (100).JPG\n",
      "5_woman (101).JPG\n",
      "5_woman (102).JPG\n",
      "5_woman (103).JPG\n",
      "5_woman (104).JPG\n",
      "5_woman (105).JPG\n",
      "5_woman (106).JPG\n",
      "5_woman (107).JPG\n",
      "5_woman (108).JPG\n",
      "5_woman (109).JPG\n",
      "5_woman (11).JPG\n",
      "5_woman (110).JPG\n",
      "5_woman (111).JPG\n",
      "5_woman (112).JPG\n",
      "5_woman (113).JPG\n",
      "5_woman (114).JPG\n",
      "5_woman (115).JPG\n",
      "5_woman (116).JPG\n",
      "5_woman (117).JPG\n",
      "5_woman (118).JPG\n",
      "5_woman (119).JPG\n",
      "5_woman (12).JPG\n",
      "5_woman (120).JPG\n",
      "5_woman (121).JPG\n",
      "5_woman (122).JPG\n",
      "5_woman (123).JPG\n",
      "5_woman (124).JPG\n",
      "5_woman (125).JPG\n",
      "5_woman (126).JPG\n",
      "5_woman (127).JPG\n",
      "5_woman (128).JPG\n",
      "5_woman (129).JPG\n",
      "5_woman (13).JPG\n",
      "5_woman (130).JPG\n",
      "5_woman (131).JPG\n",
      "5_woman (132).JPG\n",
      "5_woman (14).JPG\n",
      "5_woman (15).JPG\n",
      "5_woman (16).JPG\n",
      "5_woman (17).JPG\n",
      "5_woman (18).JPG\n",
      "5_woman (19).JPG\n",
      "5_woman (2).JPG\n",
      "5_woman (20).JPG\n",
      "5_woman (21).JPG\n",
      "5_woman (22).JPG\n",
      "5_woman (23).JPG\n",
      "5_woman (24).JPG\n",
      "5_woman (25).JPG\n",
      "5_woman (26).JPG\n",
      "5_woman (27).JPG\n",
      "5_woman (28).JPG\n",
      "5_woman (29).JPG\n",
      "5_woman (3).JPG\n",
      "5_woman (30).JPG\n",
      "5_woman (31).JPG\n",
      "5_woman (32).JPG\n",
      "5_woman (33).JPG\n",
      "5_woman (34).JPG\n",
      "5_woman (35).JPG\n",
      "5_woman (36).JPG\n",
      "5_woman (37).JPG\n",
      "5_woman (38).JPG\n",
      "5_woman (39).JPG\n",
      "5_woman (4).JPG\n",
      "5_woman (40).JPG\n",
      "5_woman (41).JPG\n",
      "5_woman (42).JPG\n",
      "5_woman (43).JPG\n",
      "5_woman (44).JPG\n",
      "5_woman (45).JPG\n",
      "5_woman (46).JPG\n",
      "5_woman (47).JPG\n",
      "5_woman (48).JPG\n",
      "5_woman (49).JPG\n",
      "5_woman (5).JPG\n",
      "5_woman (50).JPG\n",
      "5_woman (51).JPG\n",
      "5_woman (52).JPG\n",
      "5_woman (53).JPG\n",
      "5_woman (54).JPG\n",
      "5_woman (55).JPG\n",
      "5_woman (56).JPG\n",
      "5_woman (57).JPG\n",
      "5_woman (58).JPG\n",
      "5_woman (59).JPG\n",
      "5_woman (6).JPG\n",
      "5_woman (60).JPG\n",
      "5_woman (61).JPG\n",
      "5_woman (62).JPG\n",
      "5_woman (63).JPG\n",
      "5_woman (64).JPG\n",
      "5_woman (65).JPG\n",
      "5_woman (66).JPG\n",
      "5_woman (67).JPG\n",
      "5_woman (68).JPG\n",
      "5_woman (69).JPG\n",
      "5_woman (7).JPG\n",
      "5_woman (70).JPG\n",
      "5_woman (71).JPG\n",
      "5_woman (72).JPG\n",
      "5_woman (73).JPG\n",
      "5_woman (74).JPG\n",
      "5_woman (75).JPG\n",
      "5_woman (76).JPG\n",
      "5_woman (77).JPG\n",
      "5_woman (78).JPG\n",
      "5_woman (79).JPG\n",
      "5_woman (8).JPG\n",
      "5_woman (82).JPG\n",
      "5_woman (85).JPG\n",
      "5_woman (86).JPG\n",
      "5_woman (87).JPG\n",
      "5_woman (88).JPG\n",
      "5_woman (89).JPG\n",
      "5_woman (9).JPG\n",
      "5_woman (90).JPG\n",
      "5_woman (91).JPG\n",
      "5_woman (92).JPG\n",
      "5_woman (93).JPG\n",
      "5_woman (94).JPG\n",
      "5_woman (95).JPG\n",
      "5_woman (96).JPG\n",
      "5_woman (97).JPG\n",
      "5_woman (98).JPG\n",
      "5_woman (99).JPG\n"
     ]
    }
   ],
   "source": [
    "# The Fourier elliptical features are extracted from each of the images and we proceed to save them in a. txt file.\n",
    "base = \"./\"\n",
    "def EF_oper(img_rgb):\n",
    "    img_binary = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary = cv2.threshold(img_binary, 127, 255, 0)\n",
    "    contours, _ = cv2.findContours(img_binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    maxcontour = max(contours, key=cv2.contourArea)\n",
    "    # cv2.drawContours(img_rgb,[maxcontour],0,(255,9,9))\n",
    "    # plt_t('cont',img_rgb)\n",
    "    coeffs = []\n",
    "    # Find the coefficients of all contours\n",
    "    coeffs.append(elliptic_fourier_descriptors(np.squeeze(maxcontour), order=13))\n",
    "    # print(\"coeff\",coeffs)\n",
    "    coeffs2 = []\n",
    "    for row in coeffs:\n",
    "        for elem in row:\n",
    "            coeffs2.append(elem)\n",
    "    coeffs = []\n",
    "    for row in coeffs2:\n",
    "        for elem in row:\n",
    "            coeffs.append(elem)\n",
    "    return np.array(coeffs)\n",
    "\n",
    "\n",
    "def EllipticFourier():\n",
    "    print(\"EF\\n\")\n",
    "    path_EF = base + r\"Feature-Extraction\"\n",
    "    if not os.path.exists(path_EF):\n",
    "        os.makedirs(path_EF)\n",
    "\n",
    "    file = open(base + r\"Feature-Extraction/Elliptic-Fourier.txt\", \"w\")\n",
    "    # file = open(r\"C:\\Users\\Ever\\Desktop\\Elliptic-Fourier.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation-masks\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                coeffs = EF_oper(img_binary)\n",
    "                file.write(name)\n",
    "                for item in range(len(coeffs)):\n",
    "                    file.write(\",%.4f\" % coeffs[item])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "\n",
    "    file.close()\n",
    "EllipticFourier()\n",
    "# img_binary = cv2.imread(r'Image-Segmentation2\\0_men (4).JPG')\n",
    "# coeffs = EF_oper(img_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def extract_sift_features(img):\n",
    "    # image_descriptors = []\n",
    "    # sift = cv2.xfeatures2d.SIFT_create()\n",
    "    orb = cv2.ORB_create()\n",
    "    _, descriptors = orb.detectAndCompute(img, None)\n",
    "    # image_descriptors.append(descriptor)\n",
    "    return descriptors\n",
    "\n",
    "def kmean_bow(all_descriptors, num_cluster):\n",
    "    bow_dict = []\n",
    "    kmeans = KMeans(n_clusters = num_cluster)\n",
    "    kmeans.fit(all_descriptors)\n",
    "\n",
    "    bow_dict = kmeans.cluster_centers_\n",
    "\n",
    "    if not os.path.isfile('./Feature-Extraction/bow_dictionary.pkl'):\n",
    "        pickle.dump(bow_dict, open('./Feature-Extraction/bow_dictionary.pkl', 'wb'))\n",
    "    return bow_dict\n",
    "\n",
    "def create_feature_bow(image_descriptors, BoW, num_cluster):\n",
    "\n",
    "    X_features = []\n",
    "\n",
    "    for i in range(len(image_descriptors)):\n",
    "        features = np.array([0] * num_cluster)\n",
    "\n",
    "        if image_descriptors[i] is not None:\n",
    "            distance = cdist(image_descriptors[i], BoW)\n",
    "\n",
    "            argmin = np.argmin(distance, axis = 1)\n",
    "\n",
    "            for j in argmin:\n",
    "                features[j] += 1\n",
    "        X_features.append(features)\n",
    "\n",
    "    return X_features\n",
    "\n",
    "\n",
    "# The Histogram of Oriented Gradients features are extracted from each of the images and we proceed to save them in a. txt file\n",
    "def SIFT():\n",
    "    print(\"SIFT\\n\")\n",
    "    file  = open(r\"./Feature-Extraction/SIFT.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    all_descriptors = []\n",
    "    image_desctiptors = []\n",
    "    all_names = []\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                print(name)\n",
    "                all_names.append(name)\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                descriptors = extract_sift_features(img_binary)\n",
    "\n",
    "\n",
    "                # Load SIFT descriptors\n",
    "                # descriptors = np.load(\"descriptors.npy\")\n",
    "\n",
    "                # Normalize descriptors\n",
    "                scaler = StandardScaler()\n",
    "                descriptors_norm = scaler.fit_transform(descriptors)\n",
    "\n",
    "                # Cluster descriptors using k-means\n",
    "                kmeans = KMeans(n_clusters=100)\n",
    "                kmeans.fit(descriptors_norm)\n",
    "\n",
    "                # Compute visual word histograms for each image\n",
    "                labels = kmeans.predict(scaler.transform(descriptors))\n",
    "\n",
    "                # Compute histogram of visual words\n",
    "                histogram, _ = np.histogram(labels, bins=range(100), density=True)\n",
    "\n",
    "\n",
    "                # image_desctiptors.append(desc)\n",
    "    #             kmeans = KMeans(n_clusters=50)\n",
    "    #             kmeans.fit(desc)\n",
    "    # #             create histogram\n",
    "    #             histogram1 = np.zeros(100)\n",
    "    #             for descriptor in desc:\n",
    "    #                 index = kmeans.predict([descriptor.astype(np.double)])\n",
    "    #                 histogram1[index] += 1\n",
    "                file.write(name)\n",
    "                for item in range(len(histogram)):\n",
    "                    file.write(\",%.3f\" % histogram[item])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "\n",
    "    # all_descriptors = []\n",
    "    # for descriptor in image_desctiptors:\n",
    "    #     if descriptor is not None:\n",
    "    #         for des in descriptor:\n",
    "    #             all_descriptors.append(des)\n",
    "                \n",
    "                \n",
    "    # num_cluster = 150\n",
    "    # BoW = kmean_bow(all_descriptors, num_cluster)\n",
    "    # X_features = create_feature_bow(image_desctiptors, BoW, num_cluster)\n",
    "    # print(len(X_features))\n",
    "    # print(X_features[0])\n",
    "    \n",
    "    # for i in range(len(all_names)):\n",
    "    #     file.write(all_names[i])\n",
    "    #     for item in range(len(X_features[i])):\n",
    "    #         file.write(\",%.3f\" % X_features[i][item])\n",
    "    #     file.write(\",\" + all_names[i][0] + \"\\n\")\n",
    "    # file.close()\n",
    "    \n",
    "SIFT()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAM9Kz24QRs6",
    "outputId": "8e09888e-414c-4b43-9bc3-93cbaa80de43",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def HOG_oper(img_binary):\n",
    "    (H) = feature.hog(img_binary, orientations=9, pixels_per_cell=(16,16),\n",
    "                                  cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\",channel_axis=-1) \n",
    "    pca = joblib.load(r\"./Feature-Extraction/pca.pkl\") #PCA(0.97).fit_transform(H.reshape(1, -1))\n",
    "    components = pca.transform(H.reshape(1, -1))\n",
    "    # joblib.dump(pca, r\"./Feature-Extraction/pca.pkl\")\n",
    "    return components\n",
    "\n",
    "def HOG_PCA():\n",
    "    data_HOG = pd.read_csv(r'./Feature-Extraction/Histogram-of-Oriented-Gradients.txt', sep=',', header=None)\n",
    "    file2 = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients-PCA.txt\", \"w\")\n",
    "    name_HOG = data_HOG.iloc[:, 0]\n",
    "    value_HOG = data_HOG.iloc[:, 1:-1]\n",
    "    tag_HOG = data_HOG.iloc[:, -1] # 0,1,2,3,4,5\n",
    "    print(\"PCA\")\n",
    "    pca = PCA(0.97).fit(value_HOG)\n",
    "    joblib.dump(pca, r\"./Feature-Extraction/pca.pkl\")\n",
    "    \n",
    "    components = pca.transform(value_HOG)\n",
    "    print(components.shape)\n",
    "    for row in range(len(components)):\n",
    "        file2.write(name_HOG[row])\n",
    "        for colm in range(len(components[row])):\n",
    "            file2.write(\",%.4f\" %components[row][colm])\n",
    "        file2.write(\",%s\" %tag_HOG[row] + \"\\n\")\n",
    "    file2.close()\n",
    "\n",
    "# The Histogram of Oriented Gradients features are extracted from each of the images and we proceed to save them in a. txt file\n",
    "# def HOG():\n",
    "#     print(\"HOG\\n\")\n",
    "#     file  = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients.txt\", \"w\")\n",
    "#     lstFiles = []  # nombre de imagenes\n",
    "#     path = r\"./Image-Segmentation2\"\n",
    "#     for (path, _, archivos) in walk(path):\n",
    "#         for arch in archivos:\n",
    "#             (nomArch, ext) = os.path.splitext(arch)\n",
    "#             if (ext == \".JPG\"):\n",
    "#                 lstFiles.append(nomArch + ext)\n",
    "#                 direc = path + \"/\" + nomArch + ext\n",
    "#                 name = nomArch + ext\n",
    "#                 # print(nomArch + ext)\n",
    "#                 img_binary = cv2.imread(direc)\n",
    "                \n",
    "#                 (H) = feature.hog(img_binary, orientations=9, pixels_per_cell=(16,16),\n",
    "#                                   cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\",channel_axis=-1)  # ,visualize=True\n",
    "#                 # hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255))     ,hogImage\n",
    "#                 # hogImage = hogImage.astype(\"uint8\")\n",
    "                \n",
    "#                 # plt.imshow(\"HOG Image\", hogImage)\n",
    "#                 file.write(name)\n",
    "#                 for item in range(len(H)):\n",
    "#                     file.write(\",%.3f\" % H[item])\n",
    "#                 file.write(\",\" + name[0] + \"\\n\")\n",
    "#     file.close()\n",
    "\n",
    "\n",
    "\n",
    "def HOG():\n",
    "    print(\"HOG\\n\")\n",
    "    file  = open(r\"./Feature-Extraction/Histogram-of-Oriented-Gradients.txt\", \"w\")\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "\n",
    "                image = cv2.imread(\"Image-Segmentation2/\" + arch)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "                image = cv2.resize(image, (461, 260))\n",
    "                \n",
    "                (H, hogImage) = feature.hog(image, orientations=9,  pixels_per_cell=(32, 32), cells_per_block=(2, 2), transform_sqrt=True, block_norm=\"L1\", visualize=True, feature_vector=True)\n",
    "\n",
    "\n",
    "                file.write(name)\n",
    "                for item in range(len(H)):\n",
    "                    file.write(\",%.3f\" % H[item])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "    \n",
    "HOG()\n",
    "HOG_PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rcWQ_TIEQRs6",
    "outputId": "b2d393be-96d8-4900-8655-18f7a5cf26d2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The Hu Moments features are extracted from each of the images and we proceed to save them in a. txt file\n",
    "# %pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def HU_oper(img_binary):\n",
    "    # https://www.pyimagesearch.com/2014/10/27/opencv-shape-descriptor-hu-moments-example/\n",
    "    img_binary = cv2.cvtColor(img_binary, cv2.COLOR_BGR2GRAY)\n",
    "    HU = cv2.HuMoments(cv2.moments(img_binary)).flatten()\n",
    "    return HU\n",
    "\n",
    "def HU():\n",
    "    print(\"HU\\n\")\n",
    "\n",
    "    file = open(\"./Feature-Extraction/Hu-Moments.txt\", \"w\")\n",
    "\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in tqdm(walk(path)):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                #print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                \n",
    "                HU = HU_oper(img_binary)\n",
    "                file.write(name)\n",
    "                for item in range(len(HU)):\n",
    "                    # print(HU[item])\n",
    "                    num = str(HU[item])\n",
    "                    file.write(\",%s\" % num[0:25])\n",
    "                    # print(num[0:22])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "    \n",
    "    file = open(r\"./Feature-Extraction/Hu-Moments-Nmz.txt\", \"w\")\n",
    "\n",
    "    data = pd.read_csv(r'./Feature-Extraction/Hu-Moments.txt', sep=',', header=None)\n",
    "\n",
    "    name = data.iloc[:, 0]\n",
    "    value = data.iloc[:, 1:-1]\n",
    "    tag = data.iloc[:, -1]\n",
    "\n",
    "    # print(value)\n",
    "    normalizedata = normalize(value, axis=0, norm='max')\n",
    "    # print(normalizedata)\n",
    "\n",
    "\n",
    "    for row in range(len(normalizedata)):\n",
    "        file.write(name[row])\n",
    "        for colm in range(len(normalizedata[row])):\n",
    "            # print(HU[item])\n",
    "            num = str(normalizedata[row][colm])\n",
    "            file.write(\",%s\" % num[0:25])\n",
    "            # print(num[0:22])\n",
    "        file.write(\",\" + str(tag[row]) + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "# HU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmKHmlXWQRs7",
    "outputId": "41b9b1f0-e30b-4ab5-9213-4167c3667a5b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    normalized_X = (X - mu) / sigma\n",
    "    \n",
    "    return (normalized_X, mu, sigma)\n",
    "\n",
    "def GM_oper(img_binary):\n",
    "    img_binary = cv2.cvtColor(img_binary, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ret, img_binary = cv2.threshold(img_binary, 127, 255, 0)\n",
    "    contours, hierarchy = cv2.findContours(img_binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    # reshape contour to 2d array\n",
    "    # cnt = cnt.reshape(cnt.shape[0], cnt.shape[2])\n",
    "    print(cnt.shape)\n",
    "    # apply PCA to contour and keep first 25 components\n",
    "    # pca = PCA(n_components=2)\n",
    "    # cnt_pca = pca.fit_transform(cnt).flatten()\n",
    "    # print(cnt_pca.shape)\n",
    "    # return cnt_pca\n",
    "    # print(cnt.shape)\n",
    "    # # Area\n",
    "    # area = cv2.contourArea(cnt)\n",
    "    # # Perimetro\n",
    "    # perimeter = cv2.arcLength(cnt, True)\n",
    "\n",
    "    # # Relación de aspecto\n",
    "    # x, y, w, h = cv2.boundingRect(cnt)\n",
    "    # aspect_ratio = float(w) / h\n",
    "\n",
    "    # # Grado\n",
    "    # rect_area = w * h\n",
    "    # extent = float(area) / rect_area\n",
    "\n",
    "    # # ConvexHull\n",
    "    # hull = cv2.convexHull(cnt)\n",
    "    # hull_area = cv2.contourArea(hull)\n",
    "\n",
    "    # # Solidez\n",
    "    # solidity = float(area) / hull_area\n",
    "\n",
    "    # # Diámetro equivalente\n",
    "    # equi_diameter = np.sqrt(4 * area / np.pi)\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "\n",
    "# The Contour Features are extracted from each of the images and we proceed to save them in a. txt file\n",
    "def GM():\n",
    "    print(\"GM\\n\")\n",
    "\n",
    "    file = open(\"./Feature-Extraction/Geometric.txt\", \"w\")\n",
    "\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                # print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                cnt_pca = GM_oper(img_binary)\n",
    "                print(len(cnt_pca))\n",
    "                file.write(name)\n",
    "                for item in range(len(cnt_pca)):\n",
    "                    # print(HU[item])\n",
    "                    num = str(cnt_pca[item])\n",
    "                    file.write(\",%s\" % num)\n",
    "                    # print(num[0:22])\n",
    "                # file.write(\",\" + str(cnt_pca) + \"\\n\")\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "                # file.write(\",%.4f\" % area)\n",
    "                # file.write(\",%.4f\" % perimeter)\n",
    "                # file.write(\",%.4f\" % aspect_ratio)\n",
    "                # file.write(\",%.4f\" % extent)\n",
    "                # file.write(\",%.4f\" % hull_area)\n",
    "                # file.write(\",%.4f\" % solidity)\n",
    "                # file.write(\",%.4f\" % equi_diameter)\n",
    "                # file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "GM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYciicz8QRs7",
    "outputId": "e5d39aa7-8d77-4bee-a1b1-a56cce8a7bb9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cof\n",
      "\n",
      "EF:  (1803, 52)\n",
      "HOG:  (1803, 6804)\n"
     ]
    }
   ],
   "source": [
    "# # After obtaining the characteristics individually we proceed to make a unification \n",
    "def combineFeatures(preprocced_img):\n",
    "    value_EF = EF_oper(preprocced_img)\n",
    "    print(\"size EF: \",value_EF.shape)\n",
    "    # value_HU = HU_oper(preprocced_img)\n",
    "    # print(\"size HU: \",value_HU.shape)\n",
    "    # normalized_HU = normalize(value_HU.reshape(-1,1),norm='max')\n",
    "    # print(\"size HU: \",normalized_HU.shape)\n",
    "    # value_GM = GM_oper(preprocced_img)\n",
    "    # print(\"size GM: \",len(value_GM))\n",
    "    VALUR_HOG = HOG_oper(preprocced_img)\n",
    "    print(\"size HOG: \",VALUR_HOG.shape)\n",
    "\n",
    "    # combine all values \n",
    "    value = np.concatenate((value_EF, VALUR_HOG), axis=None)\n",
    "    return value\n",
    "\n",
    "def CoF():\n",
    "    print(\"Cof\\n\")\n",
    "\n",
    "    file = open(\"./Feature-Extraction/CoF.txt\", \"w\")\n",
    "    file2 = open(\"./Feature-Extraction/HOG_EF.txt\", \"w\")\n",
    "\n",
    "    data_EF = pd.read_csv(r'./Feature-Extraction/Elliptic-Fourier.txt', sep=',', header=None)\n",
    "    name_EF = data_EF.iloc[:, 0]\n",
    "    value_EF = data_EF.iloc[:, 1:-1]\n",
    "    tag_EF = data_EF.iloc[:, -1]\n",
    "    print(\"EF: \",value_EF.shape)\n",
    "    # -------------------------------- HM---------------------------------#\n",
    "    # data_HM = pd.read_csv(base + r'Feature-Extraction/Hu-Moments.txt', sep=',', header=None)\n",
    "    # value_HM = data_HM.iloc[:, 1:-1]\n",
    "    # normalizedata = normalize(value_HM, axis=0, norm='max')\n",
    "    # print(\"HM: \",normalizedata.shape)\n",
    "    # -------------------------------- GM---------------------------------#\n",
    "    # data_GM = pd.read_csv(base + r'Feature-Extraction/Geometric.txt', sep=',', header=None)\n",
    "    # value_GM = data_GM.iloc[:, 1:-1]\n",
    "    # print(\"GM: \",value_GM.shape)\n",
    "    # -------------------------------- HOG---------------------------------#\n",
    "    data_HOG = pd.read_csv(base + r'Feature-Extraction/Histogram-of-Oriented-Gradients.txt', sep=',', header=None)\n",
    "    value_HOG = data_HOG.iloc[:, 1:-1]\n",
    "    print(\"HOG: \",value_HOG.shape)\n",
    "    # -------------------------------- Save Cof ---------------------------------#\n",
    "    \n",
    "    for row in range(len(value_EF)):\n",
    "        # file.write(name_EF[row])\n",
    "        file2.write(name_EF[row])\n",
    "        for colm in range(value_EF.shape[1]):\n",
    "            # file.write(\",%.4f\" %value_EF.iloc[row,colm])\n",
    "            file2.write(\",%.4f\" %value_EF.iloc[row,colm])\n",
    "        # for colm in range(len(normalizedata[row])):\n",
    "        #     num = str(normalizedata[row][colm])\n",
    "        #     file.write(\",%s\" % num[0:25])\n",
    "        # for colm in range(value_GM.shape[1]):\n",
    "        #     file.write(\",%.4f\" %value_GM.iloc[row,colm])\n",
    "        for colm in range(value_HOG.shape[1]):\n",
    "            # file.write(\",%.4f\" %value_HOG.iloc[row,colm])\n",
    "            file2.write(\",%.4f\" %value_HOG.iloc[row,colm])\n",
    "        file.write(\",%s\" %tag_EF[row] + \"\\n\")\n",
    "        file2.write(\",%s\" %tag_EF[row] + \"\\n\")\n",
    "        \n",
    "    file.close()\n",
    "    file2.close()\n",
    "\n",
    "CoF()\n",
    "# EF:  (1827, 52)\n",
    "# HM:  (1827, 7)\n",
    "# GM:  (1827, 7)\n",
    "# HOG:  (1827, 1250)\n",
    "\n",
    "# img = preprocess(r\"dataset\\men\\0\\0_men (7).JPG\")\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "# value = HOG_oper(img)\n",
    "# print(value.shape)\n",
    "\n",
    "# # load model and predict class\n",
    "# clf = joblib.load(r'Feature-Extraction\\SVM\\modelo_entrenado-Histogram-of-Oriented-Gradients-PCA-20%.pkl')\n",
    "# # clf.predict(value)\n",
    "\n",
    "# # loop over the testing images in us folder\n",
    "\n",
    "# path = r\"./dataset/\"\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for (path, _, archivos) in walk(path):\n",
    "#     for arch in archivos:\n",
    "#             (nomArch, ext) = os.path.splitext(arch)\n",
    "#             if (ext == \".JPG\"):\n",
    "#                 # load the image, convert it to grayscale, and resize it to be a fixed\n",
    "#                 # 64x64 pixels, ignoring aspect ratio\n",
    "#                 direc = path + \"/\" + nomArch + ext\n",
    "#                 # print(direc)\n",
    "#                 image = preprocess(direc)\n",
    "#                 # plt_t(\"after preprocess\",image, cmap='gray')\n",
    "#                 image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "#                 value = HOG_oper(image)\n",
    "#                 # value = combineFeatures(image)\n",
    "#                 # load the image and classify it\n",
    "#                 # print(path[-1])\n",
    "#                 is_corr = 1 if (clf.predict(value)[0]) == path[-1] else 0\n",
    "#                 if(is_corr == 0):\n",
    "#                     print(\"incorrect: \",direc)\n",
    "#                     plt_t(\"incorrect\",image, cmap='gray')\n",
    "                 \n",
    "#                 correct += is_corr\n",
    "#                 total+=1\n",
    "#                 # show the prediction\n",
    "# print(\"correct: \",correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PCA \n",
    "\n",
    "def featureNormalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    normalized_X = (X - mu) / sigma\n",
    "    \n",
    "    return (normalized_X, mu, sigma)\n",
    "\n",
    "\n",
    "\n",
    "def PCAA():\n",
    "    print(\"HU\\n\")\n",
    "\n",
    "    file = open(\"./Feature-Extraction/pca.txt\", \"w\")\n",
    "\n",
    "    lstFiles = []  # nombre de imagenes\n",
    "    path = r\"./Image-Segmentation2\"\n",
    "    for (path, _, archivos) in walk(path):\n",
    "        for arch in archivos:\n",
    "            (nomArch, ext) = os.path.splitext(arch)\n",
    "            if (ext == \".JPG\"):\n",
    "                lstFiles.append(nomArch + ext)\n",
    "                direc = path + \"/\" + nomArch + ext\n",
    "                name = nomArch + ext\n",
    "                #print(nomArch + ext)\n",
    "                img_binary = cv2.imread(direc)\n",
    "                img_binary = cv2.cvtColor(img_binary, cv2.COLOR_BGR2GRAY)\n",
    "                # make returned image as float32 datatype\n",
    "                img_binary = np.float32(img_binary/255.0)\n",
    "                \n",
    "                X = normalize(img_binary, axis=0, norm='max')\n",
    "                # take the most significant 25 vectors of the image and flatten it using PCA\n",
    "                pca = PCA(n_components=25)\n",
    "                X = pca.fit_transform(X)\n",
    "                # print(X)\n",
    "                file.write(name)\n",
    "                for item in range(len(X)):\n",
    "                    for item2 in range(len(X[item])):\n",
    "                        file.write(\",%s\" % item2)\n",
    "                        # print(HU[item])\n",
    "                    # print(num[0:22])\n",
    "    #             file.write(name)\n",
    "    #             for item in range(len(HU)):\n",
    "    #                 # print(HU[item])\n",
    "    #                 num = str(HU[item])\n",
    "    #                 file.write(\",%s\" % num[0:25])\n",
    "    #                 # print(num[0:22])\n",
    "                file.write(\",\" + name[0] + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "\n",
    "PCAA()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JEB98uEZQRs7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then proceed to create our classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZkiEc6i7QRs7",
    "outputId": "c933619a-7d4c-489c-d3e2-5dd0c232b7c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#As the first method of classification we use Support Vector Machine \n",
    "\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "base = \"./\"\n",
    "\n",
    "def SVM(txt,test_percentage):\n",
    "    pathsvm = base + \"Feature-Extraction/SVM\"\n",
    "    \n",
    "    if not os.path.exists(pathsvm):\n",
    "        os.makedirs(pathsvm)\n",
    "        \n",
    "    data = pd.read_csv(base + '/Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    \n",
    "    # we shuffle it for better performance \n",
    "    # data=shuffle(data, random_state=42)\n",
    "    \n",
    "    s=data.shape\n",
    "    # print(s)\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "     \n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "    \n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "    \n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "    \n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "    \n",
    "    #print(data.tail())\n",
    "    \n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    allValuesName = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[allValuesName]\n",
    "    # print value type\n",
    "    # print(type(value))\n",
    "    # convert to float\n",
    "    # value=value.astype(float)\n",
    "    \n",
    "    tags=data[col[-1]] #columna de tags\n",
    "    \n",
    "    data['gender'] = data['NAME'].map(lambda x: 'woman' in x.lower())\n",
    "    \n",
    "    # i added a stratify to the train_test_split to make sure that the train and test sets have the same proportion of class labels as the input data\n",
    "    # its based on gender\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test_percentage,stratify=data['gender'], random_state=42)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=test_percentage,stratify=Y_train, random_state=42)\n",
    "    C_range=[0.01, 0.1, 1, 10, 100, 1000]\n",
    "    gamma_range=[1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5]\n",
    "    parameters= [\n",
    "        {\n",
    "            'kernel': ['rbf'],\n",
    "            'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "            'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "        }#, \n",
    "        #{\n",
    "        #    'kernel': ['linear'],\n",
    "        #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        #}, \n",
    "        #{\n",
    "        #    'kernel': ['sigmoid'],\n",
    "        #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "        #    'C': [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "        #}, \n",
    "        # {\n",
    "        #    'kernel': ['poly'],\n",
    "        #    'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "        #    'C': C_range\n",
    "        # }\n",
    "        \n",
    "    ]\n",
    "    # \n",
    "    clf =GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=3)\n",
    "    # pipe = Pipeline([('scaler', StandardScaler()), ('svm', clf)])\n",
    "    clf.fit(X_train.values,Y_train)\n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    plt.xlabel('Gamma')\n",
    "    plt.ylabel('C')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    fig=plt.title('Heat map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    fig.get_figure().savefig(base + r'Feature-Extraction/SVM/Heatmap-'+txt+'-'+str(int(test_percentage*100))+'%.jpg')\n",
    "    plt.show()\n",
    "    print(clf.best_params_)#mejor parametro\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for m, s, p in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "    y_pred = clf.predict(X_val)\n",
    "    \n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    \n",
    "    table=classification_report(Y_val,y_pred, target_names=target_names)\n",
    "    table=str(table)\n",
    "    print(table)\n",
    "    file = open(base +r\"Feature-Extraction/SVM/Reports.txt\", \"a+\")\n",
    "    file.write(txt+'-'+str(int(test_percentage*100))+'%\\n\\n')\n",
    "    file.write(table+'\\n')\n",
    "    file.write(\"Accuracy: \"+str(clf.score(value,tags)))\n",
    "    mat=confusion_matrix(Y_val, y_pred)\n",
    "    \n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "    \n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    \n",
    "    Matrizconf.get_figure().savefig(base + r'Feature-Extraction/SVM/Confusionmap-'+txt+'-'+str(int(test_percentage*100))+'%.png')\n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_val.groupby(Y_val).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "    joblib.dump(clf,base +r'Feature-Extraction/SVM/modelo_entrenado-'+txt+'-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    \n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(clf.score)\n",
    "    print(\"Accuracy: \"+str(clf.score(X_test,Y_test)))\n",
    "    file.close()\n",
    "    \n",
    "    print(\"Accuracy: \"+str(clf.score(value,tags)))\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# SVM(\"Elliptic-Fourier\",0.2)\n",
    "SVM(\"Geometric\" ,0.2)\n",
    "# SVM(\"HOG_EF\" ,0.2)\n",
    "# SVM(\"Cof\" ,0.2)\n",
    "# SVM(\"VHIST\" ,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7utyeV3fQRs8",
    "outputId": "81cd0534-eb59-450f-e8cf-1da64d07bb0b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#As a second method of classification we use K-Nearest Neighbour Classifier\n",
    "def KNN(txt,test):\n",
    "       \n",
    "    pathknn = base +\"Feature-Extraction/KNN\"\n",
    "    if not os.path.exists(pathknn):\n",
    "        os.makedirs(pathknn)\n",
    "        \n",
    "    data = pd.read_csv(base +'Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    data=shuffle(data, random_state=0)\n",
    "    \n",
    "    s=data.shape# tamaño de dataframe\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "     \n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "    \n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "    \n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "    \n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "    \n",
    "    #print(data.tail())\n",
    "    \n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    Name_value = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[Name_value]\n",
    "    \n",
    "    tags=data[col[-1]] #columna de tags\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test, random_state=0)\n",
    "    C_range=[1,2,5,25,50,100]\n",
    "    gamma_range=[1,2,5,10]\n",
    "    parameters= [\n",
    "        {\n",
    "            'n_neighbors': [1,2,5,25,50,100],\n",
    "            'metric': ['minkowski'],\n",
    "            'p': [1,2,5,10]\n",
    "        }        \n",
    "    ]\n",
    "    \n",
    "    clf = GridSearchCV(KNeighborsClassifier(), param_grid=parameters, cv=5)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    plt.xlabel('P')\n",
    "    plt.ylabel('N neighbors')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    fig=plt.title('Heat map '+txt+'-'+str(int(test*100))+'%')\n",
    "    fig.get_figure().savefig(base + 'Feature-Extraction/KNN/Heatmap-'+txt+'-'+str(int(test*100))+'%.jpg')\n",
    "    plt.show()\n",
    "    print(clf.best_params_)#mejor parametro\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for m, s, p in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    \n",
    "    table=classification_report(Y_test,y_pred, target_names=target_names)\n",
    "    table=str(table)\n",
    "    print(table)\n",
    "    file = open(base +\"Feature-Extraction/KNN/Reports.txt\", \"a+\")\n",
    "    file.write(txt+'-'+str(int(test*100))+'%\\n\\n')\n",
    "    file.write(table+'\\n')\n",
    "    mat=confusion_matrix(Y_test, y_pred)\n",
    "    \n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "    \n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test*100))+'%')\n",
    "    \n",
    "    Matrizconf.get_figure().savefig(base + 'Feature-Extraction/KNN/Confusionmap-'+txt+'-'+str(int(test*100))+'%.png')\n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_test.groupby(Y_test).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "    joblib.dump(clf,base + 'Feature-Extraction/KNN/modelo_entrenado-'+txt+'-'+str(int(test*100))+'%.pkl')\n",
    "    \n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(clf.score(X_test,Y_test))\n",
    "    file.close()\n",
    "\n",
    "\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# KNN(\"Elliptic-Fourier\",porcentaje_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PGEK89ZeQRs8",
    "outputId": "9ec65ef6-f410-4d9a-87c6-b01af6a7b413",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#As Tecer method of classification we use Neural Networks\n",
    "def NN(txt,test):\n",
    "    \n",
    "    pathnn = base + r\"Feature-Extraction/NN\"\n",
    "    if not os.path.exists(pathnn):\n",
    "        os.makedirs(pathnn)\n",
    "        \n",
    "    data = pd.read_csv(base +'Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    data=shuffle(data, random_state=0)\n",
    "    \n",
    "    s=data.shape# tamaño de dataframe\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "     \n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "    \n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "    \n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "    \n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "    \n",
    "    #print(data.tail())\n",
    "    \n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    Name_value = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[Name_value]\n",
    "    \n",
    "    tags=data[col[-1]] #columna de tags\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test, random_state=0)\n",
    "    C_range=[1,0.1,0.01,0.001,0.0001,0]\n",
    "    gamma_range=[(100,1), (100,2), (100,3)]\n",
    "    parameters= [\n",
    "        {\n",
    "            'solver':['lbfgs'], \n",
    "            'alpha':[1,0.1,0.01,0.001,0.0001,0],\n",
    "            'hidden_layer_sizes':[(100,1), (100,2), (100,3)]\n",
    "        }\n",
    "    ]\n",
    "        \n",
    "    clf =GridSearchCV(MLPClassifier(), param_grid=parameters, cv=5)\n",
    "    clf.fit(X_train,Y_train)\n",
    "    \n",
    "    scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    plt.xlabel('hidden_layer_sizes')\n",
    "    plt.ylabel('Alpha')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    fig=plt.title('Validation accuracy')\n",
    "    fig.get_figure().savefig(base + r'Feature-Extraction/NN/Heatmap-'+txt+'-'+str(int(test*100))+'%.jpg')\n",
    "    plt.show()\n",
    "    print(clf.best_params_)#mejor parametro\n",
    "    \n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    params = clf.cv_results_['params']\n",
    "    for m, s, p in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    \n",
    "    table=classification_report(Y_test,y_pred, target_names=target_names)\n",
    "    table=str(table)\n",
    "    print(table)\n",
    "    file = open(base + r\"Feature-Extraction/NN/Reports.txt\", \"a+\")\n",
    "    file.write(txt+'-'+str(int(test*100))+'%\\n\\n')\n",
    "    file.write(table+'\\n')\n",
    "    mat=confusion_matrix(Y_test, y_pred)\n",
    "    \n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "    \n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test*100))+'%')\n",
    "    \n",
    "    Matrizconf.get_figure().savefig(base + r'Feature-Extraction/NN/Confusionmap-'+txt+'-'+str(int(test*100))+'%.png')\n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_test.groupby(Y_test).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "    joblib.dump(clf,base + r'Feature-Extraction/NN/modelo_entrenado-'+txt+'-'+str(int(test*100))+'%.pkl')\n",
    "    \n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(clf.score(X_test,Y_test))\n",
    "    file.close()\n",
    "\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# NN(\"Elliptic-Fourier\",porcentaje_test[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "#As Tecer method of classification we use Neural Networks\n",
    "def RandomForest(txt,test):\n",
    "\n",
    "    pathnn = base + r\"Feature-Extraction/RNDMFOREST\"\n",
    "    if not os.path.exists(pathnn):\n",
    "        os.makedirs(pathnn)\n",
    "\n",
    "    data = pd.read_csv(base +'Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    data=shuffle(data, random_state=0)\n",
    "\n",
    "    s=data.shape# tamaño de dataframe\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "\n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "\n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "\n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "\n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "\n",
    "    #print(data.tail())\n",
    "\n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    Name_value = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[Name_value]\n",
    "\n",
    "    tags=data[col[-1]] #columna de tags\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test, random_state=0)\n",
    "\n",
    "    \n",
    "    rr = GridSearchCV(RandomForestClassifier(),param_grid={'n_estimators': [100,1000]},cv=3,verbose=0,n_jobs=-1)\n",
    "    # clf = SGDClassifier(loss='hinge')\n",
    "    clf = Pipeline(steps=[('std', StandardScaler()),('classifier', rr)])\n",
    "    clf.fit(X_train.values,Y_train)\n",
    "\n",
    "    # scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    # print(\"The best parameters are %s with a score of %0.2f\" % (clf['classifier'].best_params_, clf['classifier'].best_score_))\n",
    "    #\n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    # plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    # plt.xlabel('hidden_layer_sizes')\n",
    "    # plt.ylabel('Alpha')\n",
    "    # plt.colorbar()\n",
    "    # plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    # plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    fig=plt.title('Validation accuracy')\n",
    "    fig.get_figure().savefig(base + r'Feature-Extraction/RNDMFOREST/Heatmap-'+txt+'-'+str(int(test*100))+'%.jpg')\n",
    "    # plt.show()\n",
    "    # print(clf.best_params_)#mejor parametro\n",
    "\n",
    "    # means = clf.cv_results_['mean_test_score']\n",
    "    # stds = clf.cv_results_['std_test_score']\n",
    "    # params = clf.cv_results_['params']\n",
    "    # for m, s, p in zip(means, stds, params):\n",
    "    #     print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "\n",
    "    table=classification_report(Y_test,y_pred, target_names=target_names)\n",
    "    table=str(table)\n",
    "    print(table)\n",
    "    file = open(base + r\"Feature-Extraction/RNDMFOREST/Reports.txt\", \"a+\")\n",
    "    file.write(txt+'-'+str(int(test*100))+'%\\n\\n')\n",
    "    file.write(table+'\\n')\n",
    "    mat=confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test*100))+'%')\n",
    "\n",
    "    Matrizconf.get_figure().savefig(base + r'Feature-Extraction/RNDMFOREST/Confusionmap-'+txt+'-'+str(int(test*100))+'%.png')\n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_test.groupby(Y_test).count(),collections.Counter(y_pred))#datos de testeo\n",
    "\n",
    "    joblib.dump(clf,base + r'Feature-Extraction/RNDMFOREST/modelo_entrenado-'+txt+'-'+str(int(test*100))+'%.pkl')\n",
    "\n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(clf.score(X_test,Y_test))\n",
    "    file.close()\n",
    "\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# NN(\"Elliptic-Fourier\",porcentaje_test[1])\n",
    "RandomForest(\"Histogram-of-Oriented-Gradients-PCA\",0.2)\n",
    "# RandomForest(\"Elliptic-Fourier\",0.2)\n",
    "# RandomForest(\"pca\",0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the first method of classification we use Support Vector Machine \n",
    "\n",
    "\n",
    "base = \"./\"\n",
    "\n",
    "def SVM_PIPE(txt,test_percentage):\n",
    "    pathsvm = base + \"Feature-Extraction/SVM_PIPE/\"\n",
    "    \n",
    "    if not os.path.exists(pathsvm):\n",
    "        os.makedirs(pathsvm)\n",
    "        \n",
    "    data = pd.read_csv(base + '/Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    \n",
    "    # we shuffle it for better performance \n",
    "    # data=shuffle(data, random_state=42)\n",
    "    \n",
    "    s=data.shape\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "     \n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "    \n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "    \n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "    \n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "    \n",
    "    #print(data.tail())\n",
    "    \n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    allValuesName = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[allValuesName]\n",
    "    \n",
    "    tags=data[col[-1]] #columna de tags\n",
    "    \n",
    "    data['gender'] = data['NAME'].map(lambda x: 'woman' in x.lower())\n",
    "    \n",
    "    # i added a stratify to the train_test_split to make sure that the train and test sets have the same proportion of class labels as the input data\n",
    "    # its based on gender\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test_percentage,stratify=data['gender'], random_state=42)\n",
    "    \n",
    "    \n",
    "    \n",
    "    C_range=[0.01, 0.1, 1, 10, 100, 1000]\n",
    "    gamma_range=[1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5]\n",
    "    parameters= [\n",
    "        {\n",
    "            'kernel': ['rbf'],\n",
    "            'gamma': [1e-4, 1e-3, 1e-2, 0.1, 0.2, 0.5],\n",
    "            'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "        },\n",
    "    ]\n",
    "    # \n",
    "    # sv =GridSearchCV(svm.SVC(decision_function_shape='ovr'), param_grid=parameters,cv=2)\n",
    "    pipe = Pipeline([('scaler', StandardScaler()), ('pca', PCA(0.97)),('svm', svm.SVC())])\n",
    "    pipe.fit(X_train.values,Y_train)\n",
    "    clf = pipe['svm']\n",
    "    # clf.fit(X_train.values,Y_train)\n",
    "    # clf.best_params_ \n",
    "    # scores = clf.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "    # print(\"The best parameters are %s with a score of %0.2f\" % (clf.best_params_, clf.best_score_))\n",
    "    \n",
    "    # plt.figure(figsize=(8, 6))\n",
    "    # plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "    # plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot)\n",
    "    # plt.xlabel('Gamma')\n",
    "    # plt.ylabel('C')\n",
    "    # plt.colorbar()\n",
    "    # plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "    # plt.yticks(np.arange(len(C_range)), C_range)\n",
    "    # fig=plt.title('Heat map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    # fig.get_figure().savefig(base + r'Feature-Extraction/SVM_PIPE/Heatmap-'+txt+'-'+str(int(test_percentage*100))+'%.jpg')\n",
    "    # plt.show()\n",
    "    # print(clf.best_params_)#mejor parametro\n",
    "    \n",
    "    # means = clf.cv_results_['mean_test_score']\n",
    "    # stds = clf.cv_results_['std_test_score']\n",
    "    # params = clf.cv_results_['params']\n",
    "    # for m, s, p in zip(means, stds, params):\n",
    "    #     print(\"%0.3f (+/-%0.3f) para %r\"%(m, 2*s, p))\n",
    "    \n",
    "    y_pred = pipe.predict(X_test)\n",
    "    \n",
    "    target_names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "    \n",
    "    table=classification_report(Y_test,y_pred, target_names=target_names)\n",
    "    table=str(table)\n",
    "    print(table)\n",
    "    file = open(base +r\"Feature-Extraction/SVM_PIPE/Reports.txt\", \"a+\")\n",
    "    file.write(txt+'-'+str(int(test_percentage*100))+'%\\n\\n')\n",
    "    file.write(table+'\\n')\n",
    "    file.write(\"Accuracy: \"+str(pipe.score(X_test,Y_test)))\n",
    "    mat=confusion_matrix(Y_test, y_pred)\n",
    "    \n",
    "    Matrizconf=sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
    "               xticklabels=target_names, yticklabels= target_names )\n",
    "    \n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    mat=plt.title('Confusion map '+txt+'-'+str(int(test_percentage*100))+'%')\n",
    "    \n",
    "    Matrizconf.get_figure().savefig(base + r'Feature-Extraction/SVM_PIPE/Confusionmap-'+txt+'-'+str(int(test_percentage*100))+'%.png')\n",
    "    print(Y_train.groupby(Y_train).count())#datos de entrenamiento\n",
    "    print(Y_test.groupby(Y_test).count(),collections.Counter(y_pred))#datos de testeo\n",
    "    \n",
    "    joblib.dump(pipe,base +r'Feature-Extraction/SVM_PIPE/modelo_entrenado-'+txt+'-'+str(int(test_percentage*100))+'%.pkl')\n",
    "    \n",
    "    # se llama el modelo\n",
    "    #clf=joblib.load('modelo_entrenado.pkl')\n",
    "    # se toma todo el dataset\n",
    "    print(\"Accuracy: \"+str(pipe.score(X_test,Y_test)))\n",
    "    file.close()\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# SVM_PIPE(\"Elliptic-Fourier\",0.2)\n",
    "SVM_PIPE(\"Histogram-of-Oriented-Gradients\" ,0.2)\n",
    "# SVM(\"HOG_EF\" ,0.2)\n",
    "# SVM(\"Cof\" ,0.2)\n",
    "# SVM(\"VHIST\" ,0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#As Tecer method of classification we use Neural Networks\n",
    "def TRY_classifiers(txt,test):\n",
    "\n",
    "    pathnn = base + r\"Feature-Extraction/RNDMFOREST\"\n",
    "    if not os.path.exists(pathnn):\n",
    "        os.makedirs(pathnn)\n",
    "\n",
    "    data = pd.read_csv(base +'Feature-Extraction/'+txt+'.txt',sep=',',header=None)\n",
    "    data=shuffle(data, random_state=0)\n",
    "\n",
    "    s=data.shape# tamaño de dataframe\n",
    "    col=[]\n",
    "    #data.columns = [\"a\", \"b\", \"c\", \"etc.\"]\n",
    "\n",
    "    for x in range(0, s[1]):\n",
    "        if x==0:\n",
    "            col.append(\"NAME\")\n",
    "        elif x ==s[1]-1:\n",
    "            col.append(\"TAG\")\n",
    "        else:\n",
    "            col.append(\"VALOR-\"+str(x))\n",
    "\n",
    "    #se asigna el vector con los nombres de las columnas creado previamente y se las asignamos a la tabla\n",
    "    data.columns = col\n",
    "\n",
    "    ##print(data.groupby(['TAG'])['TAG'].count())\n",
    "    vals_to_replace = { '0':'0', '1':'1', '2':'2', '3':'3', '4':'4', '5':'5',\n",
    "                         0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5'}\n",
    "\n",
    "    data['TAG'] = data['TAG'].map(vals_to_replace)\n",
    "\n",
    "    #print(data.tail())\n",
    "\n",
    "    no_col=['NAME','TAG']\n",
    "    #obtener todas las columnas\n",
    "    Name_value = [x for x in col if x not in no_col]\n",
    "    #se obtienen solo los coefficientes\n",
    "    value=data[Name_value]\n",
    "\n",
    "    tags=data[col[-1]] #columna de tags\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(value,tags,test_size=test, random_state=0)\n",
    "\n",
    "    # try different classifiers\n",
    "    classfiers = [RandomForestClassifier(), GaussianNB(),svm.SVC(kernel='rbf'), svm.SVC(kernel='poly'),KNeighborsClassifier(),Pipeline(steps=[('scaler', StandardScaler()), ('SVC', SGDClassifier(loss=\"hinge\", penalty=\"l2\"))])]\n",
    "    names = [\"Random Forest\", \"Naive Bayes\",\"SVM-RBF\",\"SVM-POLY\",\"KNN\",\"SGDClassifier\"]\n",
    "    for clf in classfiers:\n",
    "        clf.fit(X_train, Y_train)\n",
    "        print(\"Classifier: \"+str(names[classfiers.index(clf)]))\n",
    "        print(\"Accuracy: \"+str(clf.score(X_test,Y_test)))\n",
    "        \n",
    "    \n",
    "\n",
    "# porcentaje_test=[0.30,0.25,0.20]\n",
    "# NN(\"Elliptic-Fourier\",porcentaje_test[1])\n",
    "TRY_classifiers(\"Histogram-of-Oriented-Gradients-PCA\",0.2)\n",
    "# RandomForest(\"Elliptic-Fourier\",0.2)\n",
    "# RandomForest(\"pca\",0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DzVv387QRs8",
    "outputId": "7e4f535d-ded1-4844-dbd5-96c8f611683c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #Preprocessing\n",
    "    # ImageSegmentation()\n",
    "    #Feature Extraction\n",
    "    # EllipticFourier()\n",
    "    # HOG()\n",
    "    # HU()\n",
    "    # GM()\n",
    "    # CoF()\n",
    "    porcentaje_test=[0.30,0.25,0.20]\n",
    "    #Classification\n",
    "    for j in tqdm(range(len(porcentaje_test))):\n",
    "        #Support Vector Machine\n",
    "        # SVM(\"Elliptic-Fourier\",porcentaje_test[j])\n",
    "        # SVM(\"Histogram-of-Oriented-Gradients-PCA\",porcentaje_test[j])\n",
    "        # SVM(\"Hu-Moments-Nmz\",porcentaje_test[j])\n",
    "        # SVM(\"Geometric\",porcentaje_test[j])\n",
    "        SVM(\"CoF\",porcentaje_test[j])\n",
    "        # SVM(\"HOG_EF\",porcentaje_test[j])\n",
    "        #K-Nearest Neighbors\n",
    "        # KNN(\"Elliptic-Fourier\",porcentaje_test[j])\n",
    "        # KNN(\"Histogram-of-Oriented-Gradients-PCA\",porcentaje_test[j])\n",
    "        # KNN(\"Hu-Moments-Nmz\",porcentaje_test[j])\n",
    "        # KNN(\"Geometric\",porcentaje_test[j])\n",
    "        KNN(\"CoF\",porcentaje_test[j])\n",
    "        # KNN(\"HOG_EF\",porcentaje_test[j])\n",
    "        #Neural Network\n",
    "        # NN(\"Elliptic-Fourier\",porcentaje_test[j])\n",
    "        # NN(\"Histogram-of-Oriented-Gradients-PCA\",porcentaje_test[j])\n",
    "        # NN(\"Hu-Moments-Nmz\",porcentaje_test[j])\n",
    "        # NN(\"Geometric\",porcentaje_test[j])\n",
    "        NN(\"CoF\",porcentaje_test[j])\n",
    "        # NN(\"HOG_EF\",porcentaje_test[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ55xNeTosdj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm -rf /content/kaggle/Feature-Extraction\\KNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EsYyYScwQRs8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "1. With this method for the recognition of Colombian sign language can be tried with new signs extending the dataset, also is open research because it can be tested with new methods of preprocessing, extraction of characteristics, classification being able to get to raise even more the percentages of prediction.\n",
    "\n",
    "2. According to the methods used for the extraction of characteristics, based on table 2, the characteristics of the gradient-oriented histograms (HOG) are the ones that obtained the highest percentage.\n",
    "\n",
    "3. When performing the main component analysis process, it is concluded that this process will reduce the percentage of the model's performance measure slightly.\n",
    "\n",
    "4. The geometric characteristics did not give a good result because the images contain similar characteristics such as the area or contour, this results in the model being able to predict the signs in a bad way."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
